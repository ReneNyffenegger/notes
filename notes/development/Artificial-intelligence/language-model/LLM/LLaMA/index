
LLaMA stands for → development/Artificial-intelligence/language-model/LLM/LLaMA → development/Artificial-intelligence/language-model/LLM[Large Language model] Meta AI.

{ Llama 2

 *Llama 2* is a collection of pretrained and fine-tuned LLMs with 7, 13 and 70 billion parameters. (A 34B variant was trained but not released)

  Llama 2-Chat is fine-tuned version of Llama 2 which is optimized for dialogue use cases.

  The models were trained on 2 trillion tokens using the standard *transformer architecture*.

}

sa:
  *Alpaca* is a training recipe based on the LLaMA 7B model uses the → https://en.wikipedia.org/wiki/Large_language_model#Instruction_tuning[Self-Instruct] method of instruction tuning
  to acquire capabilities comparable to the GPT-3.5 series *text-davinci-003* model at a modest cost.
