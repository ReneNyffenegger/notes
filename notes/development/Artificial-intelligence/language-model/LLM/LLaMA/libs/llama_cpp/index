$ llama.cpp

The goal of → https://github.com/ggerganov/llama.cpp[llama.cpp] is to run the → development/Artificial-intelligence/language-model/LLM/LLaMA model on a MacBook with a
→ development/languages/C-C-plus-plus[C/C++] only implementation.

{ Building the tools

  Get the sources:
code {
→ development/version-control-systems/git/commands/clone[git clone] https://github.com/ggerganov/llama.cpp
cd llama.cpp
code }

 { Windows, with MinGW

code {
P:\ath\to\llama.cpp> cmake -B build -G "MinGW Makefiles"
P:\ath\to\llama.cpp> cmake --build build --config Release
P:\ath\to\llama.cpp> build\bin\llama-cli.exe -h
…
code }

 }
 { Built executables

   After building build 4589, I found the following executables under `build/bin` (the common prefix `llama-` is removed):
   table { ll
      `batched-bench` ☰
      `batched` ☰
      `bench` ☰ Benchmark the performance of the inference for various parameters.
      `cli` ☰ A CLI tool for accessing and experimenting with most of llama.cpp's functionality.
      `convert-llama2c-to-ggml` ☰
      `cvector-generator` ☰
      `embedding` ☰
      `eval-callback` ☰
      `export-lora` ☰
      `gen-docs` ☰
      `gguf-hash` ☰
      `gguf-split` ☰
      `gguf` ☰
      `gritlm` ☰
      `imatrix` ☰
      `infill` ☰
      `llava-cli` ☰
      `lookahead` ☰
      `lookup-create` ☰
      `lookup-merge` ☰
      `lookup-stats` ☰
      `lookup` ☰
      `minicpmv-cli` ☰
      `parallel` ☰
      `passkey` ☰
      `perplexity` ☰ A tool for measuring the → https://huggingface.co/docs/transformers/perplexity[perplexity] (and other quality metrics) of a model over a given text.
      `q8dot` ☰
      `quantize` ☰
      `qwen2vl-cli` ☰
      `retrieval` ☰
      `run` ☰ A comprehensive example for running llama.cpp models. Useful for inferencing. Used with RamaLama
      `save-load-state` ☰
      `server` ☰ A lightweight, → https://github.com/openai/openai-openapi[OpenAI API] compatible, HTTP server for serving LLMs. (`llama-server -m model.gguf --port 11434`)
      `simple-chat` ☰
      `simple` ☰ A minimal example for implementing apps with llama.cpp. Useful for developers.
      `speculative-simple` ☰
      `speculative` ☰
      `tokenize` ☰
      `tts` ☰
      `vdot` ☰
   table }

   In addition, I also found the following executables (where the common prefix `test_` is omitted:
   table { ll
     `arg-parser` ☰
     `autorelease` ☰
     `backend-ops` ☰
     `barrier` ☰
     `c` ☰
     `chat-template` ☰
     `gguf` ☰
     `log` ☰
     `model-load-cancel` ☰
     `quantize-fns` ☰
     `quantize-perf` ☰
     `rope` ☰
     `tokenizer-0` ☰
   table }

 }

 { Using the (deprecated?) Makefile

code {
cd llama.cpp
→ development/make -j
code }


  Trying a model:
code {
$ curl -OL https://huggingface.co/aisuko/gpt2-117M-gguf/resolve/main/ggml-model-Q4_K_M.gguf
$ ./llama-cli -m ggml-model-Q4_K_M.gguf  -p "Tell me about programmers and coffee" -n 200
code }

  }

}
{ TODO

  { llama build number, build commit, build compiler and build target

    The values  *llama build number*, build commit, build compiler and build target is determined in

    → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh[`scripts/build-info.sh`] determines the following values:
    table { lll
      ~Value~ ☰ ~Command~ ☰ ~Example value~
      llama build number ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L10C10-L10C24[`git rev-list --count HEAD`] ☰ 4589
      build commit ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L15C10-L15C25[`git rev-parse --short HEAD`] ☰ eb7cf15a
      build compiler ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L19C10-L19C22[`$CC --version | head -1`] ☰ gcc (Debian 10.2.1-6) 10.2.1 20210110
      build target ☰  → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L23C10-L23C15[`$CC --dumpmachine`] ☰ x86_64-linux-gnu
    table }

    These value seems then to be used to produce `common/build-info.cpp` from → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/build-info.cpp.in[`common/build-info.cpp.in`].

  }
  { Environment variables

    table { ll
       `LLAMA_ARG_MODEL` ☰ Model path? (for example `models/7B/ggml-model-f16.gguf`)
       `GG_BUILD_CUDA` ☰ ?
       `GG_BUILD_SYCL` ☰ ?
       `GG_BUILD_VULKAN` ☰ ?
    table }
  
  }
  { models/7B/ggml-model-f16.gguf

   `models/7B/ggml-model-f16.gguf` is the default path where llama.cpp looks for a model if not explicitely specified, see
   → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/common.h#L25[`#define DEFAULT_MODEL_PATH "models/7B/ggml-model-f16.gguf"`]
   in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/common.h[`common/common.h`].

  }
  { ci/run.sh

    See → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/ci/README.md[`ci/README.md`].

  }

}
{ main

  2024-10-05: Apparently, `main` is deprecated in favor of `llama-cli`.

  { Options

    table { llll

☰ ☰ ~param value~ ☰
  `-h` ☰ `--help` ☰ ☰ Show this help message and exit
  `-i` ☰ `--interactive` ☰ ☰ Run in interactive mode
  ☰ `--interactive-first` ☰ ☰ Run in interactive mode and wait for input right away
  ☰ `-ins`, `--instruct` ☰  ☰ Run in instruction mode (use with Alpaca models)
  `-r` ☰ `--reverse-prompt` ☰ `PROMPT` ☰ Run in interactive mode and poll user input upon seeing `PROMPT` (can be specified more than once for multiple prompts).
  ☰ `--color` ☰  ☰ Colorise output to distinguish prompt and user input from generations
  `-s` ☰ `--seed` ☰ `SEED` ☰ Seed for random number generator (default: `-1`, use random seed for <= 0)
  `-t` ☰ `--threads` ☰ `N` ☰ Number of threads to use during computation (default: 12)
  `-p` ☰ `--prompt` ☰ `PROMPT` ☰ Prompt to start generation with (default: empty)
  ☰ `--random-prompt` ☰ ☰ Start with a randomized prompt.
  ☰ `--in-prefix` ☰ `STRING` ☰ String to prefix user inputs with (default: empty)
  `-f` ☰ `--file` ☰ `FNAME` ☰ Prompt file to start generation.
  `-n` ☰ `--n_predict` ☰ `N` ☰ Number of tokens to predict (default: 128, -1 = infinity)
   ☰ `--top_k` ☰ `N` ☰ Top-k sampling (default: 40)
 ☰ `--top_p` ☰ `N` ☰ Top-p sampling (default: 0.9)
 ☰ `--repeat_last_n` ☰ `N` ☰ Last n tokens to consider for penalize (default: 64)
 ☰ `--repeat_penalty` ☰ `N` ☰ Penalize repeat sequence of tokens (default: 1.1)
 `-c` ☰ `--ctx_size` ☰ `N` ☰ Size of the prompt context (default: `512`)
 ☰ `--ignore-eos` ☰ ☰ Ignore end of stream token and continue generating
 ☰ `--memory_f32` ☰ ☰ Use `f32` instead of `f16` for memory key+value
 ☰ `--temp` ☰ `N` ☰ Temperature (default: `0.8`)
  ☰ `--n_parts` ☰ `N` ☰ Number of model parts (default: -1 = determine from dimensions)
  `-b` ☰ `--batch_size` ☰ `N` ☰ Batch size for prompt processing (default: 8)
  ☰ `--perplexity` ☰ ☰ Compute perplexity over the prompt
  ☰ `--keep` ☰   ☰ Number of tokens to keep from the initial prompt (default: 0, -1 = all)
  ☰ `--mlock` ☰  ☰ Force system to keep model in RAM rather than swapping or compressing
  ☰ `--mtest` ☰  ☰ Determine the maximum memory usage needed to do inference for the given `n_batch` and `n_predict` parameters (uncomment the `"used_mem"` line in `llama.cpp` to see the results)
  ☰ `--verbose-prompt` ☰   ☰ Print prompt before generation
  `-m` ☰ `--model` ☰ `FNAME` ☰ Model path (default: `models/llama-7B/ggml-model.bin`)
    table }

  }

}

links:
  → https://github.com/ggerganov/ggml[`ggml`] is a tensor library, written in C, that is used in `llama.cpp`. In fact, the description of `ggml` reads:
  *Note that this project is under development and not ready for production use.  Some of the development is currently happening in the `llama.cpp` and → https://github.com/ggerganov/whisper.cpp[`whisper.cpp`] repos*

  → https://github.com/abetlen/llama-cpp-python[Python bindings for llama.cpp] provides
    • Low-level access to C API via `→ development/languages/Python/standard-library/ctypes`.
