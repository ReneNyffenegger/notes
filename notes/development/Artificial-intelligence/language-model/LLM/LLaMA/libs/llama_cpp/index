$ llama.cpp

The goal of → https://github.com/ggerganov/llama.cpp[llama.cpp] is to run the → development/Artificial-intelligence/language-model/LLM/LLaMA model on a MacBook with a
→ development/languages/C-C-plus-plus[C/C++] only implementation.

{ Building the tools

  Get the sources:
code {
→ development/version-control-systems/git/commands/clone[git clone] https://github.com/ggerganov/llama.cpp
cd llama.cpp
code }

 { Debian

   On Debian, I was able to compile the sources, as indicated in the repository's → https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#cpu-build[`README.md`] with
code {
cmake -B build
cmake --build build
code }

   The `-B` and `--build` options specify the  build directory (which is created with `-B`)

 }
 { Windows, with MinGW

   Note the `-G "MinGW Makefiles"` option:
code {
P:\ath\to\llama.cpp> cmake -B build -G "MinGW Makefiles"
P:\ath\to\llama.cpp> cmake --build build --config Release
code }

 }
 { Built executables

   After building build 4589, I found the following executables under `build/bin` (the common prefix `llama-` is removed):
   table { ll
      `batched-bench` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/batched-bench/README.md[Benchmark the batched decoding performance]
      `batched` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/batched/README.md[Demonstration of batched generation from a given prompt]. A swift clone is found under → https://github.com/ggerganov/llama.cpp/tree/master/examples/batched.swift[`examples/batched.swift`]
      `bench` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/llama-bench/README.md[Benchmark the performance] of the inference for various parameters.
      `cli` ☰ A CLI tool for accessing and → https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md[experimenting with most of llama.cpp's functionality]. TODO For the `--grammar`, `--grammar-file` and `--json-schema` command line options see `grammars` below.
      `convert-llama2c-to-ggml` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/convert-llama2c-to-ggml/README.md[This example] reads weights from Andrej Karpathy's → https://github.com/karpathy/llama2.c[llama2.c project] and saves them in ggml compatible format. The vocab that is available in `models/ggml-vocab.bin` is used by default
      `cvector-generator` ☰ Demonstration of how to → https://github.com/ggerganov/llama.cpp/blob/master/examples/cvector-generator/README.md[generate a control vector using gguf models].
      `embedding` ☰ Demonstrates the → https://github.com/ggerganov/llama.cpp/blob/master/examples/embedding/README.md[generation of a(?) high-dimensional vector of a given text]
      `eval-callback` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/eval-callback/README.md[Using callbacks during intference] to print all operations and tensor data to the console.
      `export-lora` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/export-lora/README.md[Apply LORA adapters to base models and export the resulting model]
      `gen-docs` ☰
      `gguf-hash` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/gguf-hash/README.md[Hash GGUF files] to detect difference on a per model and per tensor level.
      `gguf-split` ☰ CLI to → https://github.com/ggerganov/llama.cpp/blob/master/examples/gguf-split/README.md[split and merge GGUF files]
      `gguf` ☰
      `gritlm` ☰ Example for → https://github.com/ggerganov/llama.cpp/blob/master/examples/gritlm/README.md[Generative Representational Instruction Tuning (GRIT)]. A → https://github.com/ContextualAI/gritlm[gritlm] model can generate embeddings as well as "normal" text generation depending on the instructions in the prompt.
      `imatrix` ☰ Computes an → https://github.com/ggerganov/llama.cpp/blob/master/examples/imatrix/README.md[importance matrix for a model and given text dataset] - which Can be used during quantization to enchance the quality of the quantized models (See → https://github.com/ggerganov/llama.cpp/pull/4861[Pull Request 4861])
      `infill` ☰ → https://github.com/ggerganov/llama.cpp/tree/master/examples/infill[Using the infill mode] with Code Llama models supporting infill mode.
      `llava-cli` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README.md[LLaVA ?]
      `lookahead` ☰ Demonstration of → https://github.com/ggerganov/llama.cpp/blob/master/examples/lookahead/README.md[lookahead decoding technique], see also → https://lmsys.org/blog/2023-11-21-lookahead-decoding/[Break the Sequential Dependency of LLM Inference Using Lookahead Decoding] and → https://github.com/ggerganov/llama.cpp/pull/4207[Pull Request 4207]
      `lookup-create` ☰ Demonstration of → https://github.com/ggerganov/llama.cpp/blob/master/examples/lookup/README.md[Prompt Lookup Decoding], see also → https://github.com/apoorvumang/prompt-lookup-decoding[apoorvumang/prompt-lookup-decoding], → https://github.com/ggerganov/llama.cpp/pull/4484[Pull Reqeust 4484] and → https://github.com/ggerganov/llama.cpp/issues/4226[Issue 4226].
      `lookup-merge` ☰
      `lookup-stats` ☰
      `lookup` ☰
      `minicpmv-cli` ☰
      `parallel` ☰ (Simplified) simulation of → https://github.com/ggerganov/llama.cpp/blob/master/examples/parallel/README.md[erving incoming requests in parallel]
      `passkey` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/passkey/README.md[A passkey retrieval task] is an evaluation method used to measure a language models ability to recall information from long contexts. See also Pull Requests → https://github.com/ggerganov/llama.cpp/pull/3856[3856] and → https://github.com/ggerganov/llama.cpp/pull/4810[4810].
      `perplexity` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md[A tool] for measuring the → https://huggingface.co/docs/transformers/perplexity[perplexity] (and other quality metrics) of a model over a given text.
      `q8dot` ☰
      `quantize` ☰ → https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md[…]
      `qwen2vl-cli` ☰
      `retrieval` ☰ Demonstration of a simple → https://github.com/ggerganov/llama.cpp/blob/master/examples/retrieval/README.md[retrieval technique based on cosine similarity].
      `run` ☰ A comprehensive example for (→ https://github.com/ggerganov/llama.cpp/blob/master/examples/run/README.md[minimally]) running llama.cpp models. Useful for inferencing. Used with RamaLama
      `save-load-state` ☰
      `server` ☰ The → https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md[LLaMA.cpp HTTP Server]: A lightweight, → https://github.com/openai/openai-openapi[OpenAI API] compatible, HTTP server for serving LLMs. Based on → https://github.com/yhirose/cpp-httplib[cpp-httplib] and → https://github.com/nlohmann/json[nlohman/json]. (`llama-server -m model.gguf --port 11434`). See also `grammars/` below.
      `simple-chat` ☰ Demonstration of a → https://github.com/ggerganov/llama.cpp/blob/master/examples/simple-chat/README.md[simple chat program] using the chat template from the GGUF file.
      `simple` ☰ A → https://github.com/ggerganov/llama.cpp/blob/master/examples/simple/README.md[minimal example] for implementing apps with llama.cpp. Useful for developers.
      `speculative-simple` ☰ Demonstration of → https://github.com/ggerganov/llama.cpp/blob/master/examples/speculative-simple/README.md[basic greedy speculative decoding]
      `speculative` ☰ Demonstration of → https://github.com/ggerganov/llama.cpp/blob/master/examples/speculative/README.md[speculative decoding and tree-based speculative decoding techniques]
      `tokenize` ☰ → https://github.com/ggerganov/llama.cpp/tree/master/examples/tokenize[…]
      `tts` ☰ → https://github.com/ggerganov/llama.cpp/tree/master/examples/tts[…]
      `vdot` ☰
   table }

   In addition, I also found the following executables (Again, with the common prefix `test_` omitted):
   table { ll
     `arg-parser` ☰
     `autorelease` ☰
     `backend-ops` ☰
     `barrier` ☰
     `c` ☰
     `chat-template` ☰
     `gguf` ☰
     `log` ☰
     `model-load-cancel` ☰
     `quantize-fns` ☰
     `quantize-perf` ☰
     `rope` ☰
     `tokenizer-0` ☰
   table }

 }

 { Using the deprecated Makefile

code {
cd llama.cpp
→ development/make -j
code }

  }

  { Trying a model

    Under → https://github.com/ggerganov/llama.cpp/tree/master/models[`models/`], I found some `*.gguf` files which I believed I could use for a first test:
code {
$ build/bin/llama-cli -m models/ggml-vocab-bert-bge.gguf --prompt "tell me a nice story" --predict 100
…
llama_model_load: error loading model: missing tensor 'token_embd.weight'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'models/ggml-vocab-bert-bge.gguf'
main: error: unable to load model
code }

  Apparently, → https://github.com/ggerganov/llama.cpp/issues/3917#issuecomment-1791494136[the files under `model/` are not real models, just the vocabulary part]. The actual models need to be downloaded.

  So, I download a couple of (small) models…
code {
$ mkdir ~/llm-models
$ → tools/cURL[curl] -L https://huggingface.co/aisuko/gpt2-117M-gguf/resolve/main/ggml-model-Q4_K_M.gguf                                -o ~/llm-models/ggml-model-Q4_K_M.gguf
$ → tools/cURL[curl] -L https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf -o ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf
code }

   … show the sizes of the downloaded models …
code {
$ (cd ~/llm-models; stat --printf '%s\t%n\n' *)
4368450304      dolphin-2.2.1-mistral-7b.Q4_K_M.gguf
112858624       ggml-model-Q4_K_M.gguf
code }
 
  … and test them with the CLI tool:
code {
$ ./build/bin/llama-cli -m ~/llm-models/ggml-model-Q4_K_M.gguf                -p "Tell me about programmers and coffee" -n 200
$ ./build/bin/llama-cli -m ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf  -p "Tell me about programmers and coffee" -n 200
code }

    In Windows, with → Windows/PowerShell, a module can be downloaded with
code {
PS:> $progressPreference = 'SilentlyContinue'
PS:> → Windows/PowerShell/command-inventory/noun/webRequest/invoke https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf -outfile ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf
code }

  }
  { Testing the webserver

     A model can be run in a webserver and then accessed from a browser:
code {
$ build/bin/llama-server -m ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf
code }

     By default, the server listens on port 8080, but this can be changed like so:
code {
$ build/bin/llama-server -m ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf --port 8888
code }
    

  }
  { grammars/

    GBNF (GGML BNF) as a format to constrain the output produced by llama.cpp (like, for example: only valid JSON, or emojis)

    GBNF grammars can be useed in
      • `llama-server` (where the grammer is passed in the `grammar ` body field)
      • `llama-cli` using the `--grammar` and `--grammar-file` flags
      • → https://github.com/ggerganov/llama.cpp/tree/master/examples/gbnf-validator[`llama-gbnf-validator`]

    See also
      • `tests/test-json-schema-to-grammar.cpp` (to see which features are likely supported)
      • → https://github.com/ggerganov/llama.cpp/pull/5978
      • → https://github.com/ggerganov/llama.cpp/pull/6659
      • → https://github.com/ggerganov/llama.cpp/pull/6555

   `-j` (`--json-schema`) flag in action:
code {
llama-cli \
  -hfr bartowski/Phi-3-medium-128k-instruct-GGUF \
  -hff Phi-3-medium-128k-instruct-Q8_0.gguf \
  -j '{
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "name": {
                "type": "string",
                "minLength": 1,
                "maxLength": 100
            },
            "age": {
                "type": "integer",
                "minimum": 0,
                "maximum": 150
            }
        },
        "required": ["name", "age"],
        "additionalProperties": false
    },
    "minItems": 10,
    "maxItems": 100
  }' \
  -p 'Generate a {name, age}[] JSON array with famous actors of all ages.'
code }

    Any schema can be converted in the command line with
code {
examples/json_schema_to_grammar.py name-age-schema.json
code }

  }
  { scripts

    table { ll
      `build-info.sh` ☰
      `check-requirements.sh` ☰ checks all requirements files for each top-level convert*.py script.
      `ci-run.sh` ☰ What's the difference to `ci/run.sh`
      `compare-commits.sh` ☰ Checks out two different commits from the repository, builds the project and then runs `compare-llama-bench.py`
      `compare-llama-bench.py` ☰
      `debug-test.sh` ☰
      `gen-authors.sh` ☰ Adds new authors to the → https://github.com/ggerganov/llama.cpp/blob/master/AUTHORS[`AUTHORS`]
      `gen-unicode-data.py` ☰
      `get-flags.mk` ☰
      `get-hellaswag.sh` ☰
      `get_chat_template.py` ☰ Fetches the Jinja chat template of a HuggingFace model.
      `get-pg.sh` ☰
      `get-wikitext-103.sh` ☰
      `get-wikitext-2.sh` ☰
      `get-winogrande.sh` ☰
      `hf.sh` ☰ Dowonload a Hugging Face model (like for example `./llama-cli -m $(./scripts/hf.sh https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf)`)
      `install-oneapi.bat` ☰
      `qnt-all.sh` ☰
      `run-all-perf.sh` ☰
      `run-all-ppl.sh` ☰
      `sync-ggml-am.sh` ☰ Synchronize ggml changes to llama.cpp
      `sync-ggml.last` ☰
      `sync-ggml.sh` ☰
      `verify-checksum-models.py` ☰
      `xxd.cmake` ☰
    table }

  }
  { cmake/x64-windows-llvm.cmake

    The `cmake` directory contains a file named `x64-windows-llvm.cmake`.

    Can this file explicitely be used for an LLVM build?

  }

}
{ TODO

  { llama build number, build commit, build compiler and build target

    → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh[`scripts/build-info.sh`] determines the following values:
    table { lll
      ~Value~ ☰ ~Command~ ☰ ~Example value~
      llama build number ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L10C10-L10C24[`git rev-list --count HEAD`] ☰ 4589
      build commit ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L15C10-L15C25[`git rev-parse --short HEAD`] ☰ eb7cf15a
      build compiler ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L19C10-L19C22[`$CC --version | head -1`] ☰ gcc (Debian 10.2.1-6) 10.2.1 20210110
      build target ☰  → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L23C10-L23C15[`$CC --dumpmachine`] ☰ x86_64-linux-gnu
    table }

    These value seems then to be used to produce `common/build-info.cpp` from → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/build-info.cpp.in[`common/build-info.cpp.in`].

    However, I don't find `build-info.sh` referenced in any other file except in the → https://github.com/ggerganov/llama.cpp/blob/27d135c970c00f655d486f870edacded792bef5c/Makefile[`Makefile`] which is
    → https://github.com/ggerganov/llama.cpp/blob/27d135c970c00f655d486f870edacded792bef5c/Makefile#L2[deprecated].

    Therefore, I now believe hat these figures are determined in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/cmake/build-info.cmake[`cmake/build-info.cmake`]
    (which in turn seems to be invoked or referenced in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/cmake/build-info-gen-cpp.cmake[`common/cmake/build-info-gen-cpp.cmake`]).

  }
  { Environment variables

    table { ll
       `LLAMA_ARG_MODEL` ☰ Model path? (for example `models/7B/ggml-model-f16.gguf`)
       `GG_BUILD_CUDA` ☰ ?
       `GG_BUILD_SYCL` ☰ ?
       `GG_BUILD_VULKAN` ☰ ?
    table }
  
  }
  { models/7B/ggml-model-f16.gguf

   `models/7B/ggml-model-f16.gguf` is the default path where llama.cpp looks for a model if not explicitely specified, see
   → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/common.h#L25[`#define DEFAULT_MODEL_PATH "models/7B/ggml-model-f16.gguf"`]
   in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/common.h[`common/common.h`].

  }
  { ci/run.sh

    In addition to → https://github.com/ggerganov/llama.cpp/actions[llama.cpp's github actions], a commit to the repository triggers the execution fo → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/ci/run.sh[`ci/run.sh`].
    on dedicated cloud instances which permits heavier workloads than just Github actions.

    See → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/ci/README.md[`ci/README.md`].

    What's the difference to `scripts/ci-run.sh`

  }
  { examples/jeopardy

   `examples/jeopardy` → https://github.com/ggerganov/llama.cpp/blob/master/examples/jeopardy/README.md[is pretty much just a straight port] of → https://github.com/aigoopy/llm-jeopardy[aigoopy/llm-jeopardy]
    with an added graph viewer.

  }
  { examples/llama.android

    …

  }
  { examples/quantize-stats

    …

  }
  { examples/rpc

    → https://github.com/ggerganov/llama.cpp/blob/master/examples/rpc/README.md[The RPC server] allows running ggml backend on a remote host.

  }
  { examples/save-load-state

    …

  }
  { examples/sycl

    Example for → https://github.com/ggerganov/llama.cpp/blob/master/examples/sycl/README.md[tools for llama.cpp for SYCL on Intel GPU].

  }
  { examples/simple-cmake-pkg

    …

  }
  { examples/llama.swiftui

    Local inference of llama.cpp → https://github.com/ggerganov/llama.cpp/blob/master/examples/llama.swiftui/README.md[on an iPhone].

    See also → https://github.com/ggerganov/llama.cpp/discussions/4508[this discussion]

  }
  { Windows: PrefetchVirtualMemory unavailable

    Apparently, when compiling/running on Windows, `_WIN32_WINNT` must be at least → https://github.com/ggerganov/llama.cpp/blob/5783575c9d99c4d9370495800663aa5397ceb0be/src/llama-mmap.cpp#L383[0x602],
    otherwise, the constructor of `llama_mmap::impl` throws → https://github.com/ggerganov/llama.cpp/blob/5783575c9d99c4d9370495800663aa5397ceb0be/src/llama-mmap.cpp#L399[PrefetchVirtualMemory unavailable].

  }

}
{ main

  `main` was → https://github.com/ggerganov/llama.cpp/pull/7809[renamed to `llama-cli`] (See also → https://github.com/ggerganov/llama.cpp/blob/master/examples/deprecation-warning/README.md[`examples/deprecation-warning/README.md`].

  { Options

    table { llll

☰ ☰ ~param value~ ☰
  `-h` ☰ `--help` ☰ ☰ Show this help message and exit
  `-i` ☰ `--interactive` ☰ ☰ Run in interactive mode
  ☰ `--interactive-first` ☰ ☰ Run in interactive mode and wait for input right away
  ☰ `-ins`, `--instruct` ☰  ☰ Run in instruction mode (use with Alpaca models)
  `-r` ☰ `--reverse-prompt` ☰ `PROMPT` ☰ Run in interactive mode and poll user input upon seeing `PROMPT` (can be specified more than once for multiple prompts).
  ☰ `--color` ☰  ☰ Colorise output to distinguish prompt and user input from generations
  `-s` ☰ `--seed` ☰ `SEED` ☰ Seed for random number generator (default: `-1`, use random seed for <= 0)
  `-t` ☰ `--threads` ☰ `N` ☰ Number of threads to use during computation (default: 12)
  `-p` ☰ `--prompt` ☰ `PROMPT` ☰ Prompt to start generation with (default: empty)
  ☰ `--random-prompt` ☰ ☰ Start with a randomized prompt.
  ☰ `--in-prefix` ☰ `STRING` ☰ String to prefix user inputs with (default: empty)
  `-f` ☰ `--file` ☰ `FNAME` ☰ Prompt file to start generation.
  `-n` ☰ `--n_predict` ☰ `N` ☰ Number of tokens to predict (default: 128, -1 = infinity)
   ☰ `--top_k` ☰ `N` ☰ Top-k sampling (default: 40)
 ☰ `--top_p` ☰ `N` ☰ Top-p sampling (default: 0.9)
 ☰ `--repeat_last_n` ☰ `N` ☰ Last n tokens to consider for penalize (default: 64)
 ☰ `--repeat_penalty` ☰ `N` ☰ Penalize repeat sequence of tokens (default: 1.1)
 `-c` ☰ `--ctx_size` ☰ `N` ☰ Size of the prompt context (default: `512`)
 ☰ `--ignore-eos` ☰ ☰ Ignore end of stream token and continue generating
 ☰ `--memory_f32` ☰ ☰ Use `f32` instead of `f16` for memory key+value
 ☰ `--temp` ☰ `N` ☰ Temperature (default: `0.8`)
  ☰ `--n_parts` ☰ `N` ☰ Number of model parts (default: -1 = determine from dimensions)
  `-b` ☰ `--batch_size` ☰ `N` ☰ Batch size for prompt processing (default: 8)
  ☰ `--perplexity` ☰ ☰ Compute perplexity over the prompt
  ☰ `--keep` ☰   ☰ Number of tokens to keep from the initial prompt (default: 0, -1 = all)
  ☰ `--mlock` ☰  ☰ Force system to keep model in RAM rather than swapping or compressing
  ☰ `--mtest` ☰  ☰ Determine the maximum memory usage needed to do inference for the given `n_batch` and `n_predict` parameters (uncomment the `"used_mem"` line in `llama.cpp` to see the results)
  ☰ `--verbose-prompt` ☰   ☰ Print prompt before generation
  `-m` ☰ `--model` ☰ `FNAME` ☰ Model path (default: `models/llama-7B/ggml-model.bin`)
    table }

  }

}
{ Ollama

  → https://github.com/ollama/ollama[Ollama] is based on llama.cpp.

  { (Manual) installation

    The instructions for a → https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install[manual instruction] point to a *self-contrained binary* that can be downloaded and extracted:  
code {
$ → tools/cURL[curl] -L → https://ollama.com/download/ollama-linux-amd64.tgz -o → Linux/fhs/tmp[/tmp]/ollama-linux-amd64.tgz
code }

   Inspecting the downloaded → development/archive-files[archive file]:
code {
$ → Linux/shell/commands/tar tf /tmp/ollama-linux-amd64.tgz | → Linux/shell/commands/grep -v '/$'
./bin/ollama
./lib/ollama/libcudart.so.12
./lib/ollama/libcublasLt.so.12.4.5.8
./lib/ollama/libcudart.so.11.0
./lib/ollama/libcublas.so.11
./lib/ollama/libcublas.so.12
./lib/ollama/libcudart.so.11.3.109
./lib/ollama/libcublas.so.12.4.5.8
./lib/ollama/libcudart.so.12.4.127
./lib/ollama/libcublasLt.so.11.5.1.109
./lib/ollama/libcublasLt.so.11
./lib/ollama/runners/cuda_v11_avx/ollama_llama_server
./lib/ollama/runners/cuda_v11_avx/libggml_cuda_v11.so
./lib/ollama/runners/rocm_avx/ollama_llama_server
./lib/ollama/runners/rocm_avx/libggml_rocm.so
./lib/ollama/runners/cuda_v12_avx/ollama_llama_server
./lib/ollama/runners/cuda_v12_avx/libggml_cuda_v12.so
./lib/ollama/runners/cpu_avx2/ollama_llama_server
./lib/ollama/runners/cpu_avx/ollama_llama_server
./lib/ollama/libcublas.so.11.5.1.109
./lib/ollama/libcublasLt.so.12
code }

     These files need to be extracted under `→ Linux/fhs/usr`:
code {
$ → Linux/shell/commands/sudo → Linux/shell/commands/tar -C /usr -xzf /tmp/ollama-linux-amd64.tgz
code }

  }
  { Testing the installation

    Start the Ollama server:
code {
$ ollama serve
code }

    In another terminal:
code {
$ ollama -v
ollama version is 0.5.7
code }

    As can be seen in the terminal where the server was started, `ollama -v` basically accesses the APi `/api/version`, i. e. something like
code {
$ curl -s localhost:11434/api/version
code }

  }
  { Web API

code {
$ curl -s http://localhost:11434/api/generate -d '{
   "model" : "llama2",
   "prompt": "tell me about programmers and coffe"
}' | → development/languages/JavaScript/JSON/tools/jq

{
  "error": "model 'llama2' not found"
}
code }

   The model is not available, we need to pull it:
code {
$ curl -s http://localhost:11434/api/pull -d '{
  "model": "llama2"
}' | jq
code }

    After downloading the model, query the available models:
code {
$ curl -s http://localhost:11434/api/tags | jq -r '.models[].name'
code }

     Try again:
code { 
$ curl -s http://localhost:11434/api/generate -d '{
   "model" : "llama2",
   "prompt": "tell me about programmers and coffee"
}' | jq

{
  "error": "model requires more system memory (8.4 GiB) than is available (1.8 GiB)"
}
code }

    Same thing, but on a machine with more memory:
code {
$ curl -s http://localhost:11434/api/generate -d '{
   "model" : "llama2",
   "prompt": "tell me about programmers and coffee"
}' | jq

{
  "model": "llama2",
  "created_at": "2025-02-01T14:27:46.867393127Z",
  "response": "\n",
  "done": false
}
{
  "model": "llama2",
  "created_at": "2025-02-01T14:27:46.947556123Z",
  "response": "Program",
  "done": false
}
{
  "model": "llama2",
  "created_at": "2025-02-01T14:27:46.996489966Z",
  "response": "mers",
  "done": false
}
{
  "model": "llama2",
  "created_at": "2025-02-01T14:27:47.044551309Z",
  "response": " and",
  "done": false
}

   …

{
  "model": "llama2",
  "created_at": "2025-02-01T14:28:57.985358597Z",
  "response": "!",
  "done": false
}
{
  "model": "llama2",
  "created_at": "2025-02-01T14:28:58.038683996Z",
  "response": "",
  "done": true,
  "done_reason": "stop",
  "context": [
    518,
    25580,
    …
    26529,
    29991
  ],
  "total_duration": 33946247078,
  "load_duration": 7990899,
  "prompt_eval_count": 29,
  "prompt_eval_duration": 558000000,
  "eval_count": 703,
  "eval_duration": 33378000000
}
code }

  }
}

links:
  → https://github.com/ggerganov/ggml[`ggml`] is a tensor library, written in C, that is used in `llama.cpp`. In fact, the description of `ggml` reads:
  *Note that this project is under development and not ready for production use.  Some of the development is currently happening in the `llama.cpp` and → https://github.com/ggerganov/whisper.cpp[`whisper.cpp`] repos*

  → https://github.com/abetlen/llama-cpp-python[Python bindings for llama.cpp] provides
    • Low-level access to C API via `→ development/languages/Python/standard-library/ctypes`.
