$ llama.cpp

The goal of → https://github.com/ggerganov/llama.cpp[llama.cpp] is to run the → development/Artificial-intelligence/language-model/LLM/LLaMA model on a MacBook with a
→ development/languages/C-C-plus-plus[C/C++] only implementation.

{ Building the tools

  Get the sources:
code {
→ development/version-control-systems/git/commands/clone[git clone] https://github.com/ggerganov/llama.cpp
cd llama.cpp
code }

 { Debian

   On Debian, I was able to compile the sources, as indicated in the repository's → https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#cpu-build[`README.md`] with
code {
cmake -B build
cmake --build build
code }

   The `-B` and `--build` options specify the  build directory (which is created with `-B`)

 }
 { Windows, with MinGW

   Note the `-G "MinGW Makefiles"` option:
code {
P:\ath\to\llama.cpp> cmake -B build -G "MinGW Makefiles"
P:\ath\to\llama.cpp> cmake --build build --config Release
code }

 }
 { Built executables

   After building build 4589, I found the following executables under `build/bin` (the common prefix `llama-` is removed):
   table { ll
      `batched-bench` ☰
      `batched` ☰
      `bench` ☰ Benchmark the performance of the inference for various parameters.
      `cli` ☰ A CLI tool for accessing and experimenting with most of llama.cpp's functionality. TODO For the `--grammar`, `--grammar-file` and `--json-schema` command line options see `grammars` below.
      `convert-llama2c-to-ggml` ☰
      `cvector-generator` ☰
      `embedding` ☰
      `eval-callback` ☰
      `export-lora` ☰
      `gen-docs` ☰
      `gguf-hash` ☰
      `gguf-split` ☰
      `gguf` ☰
      `gritlm` ☰
      `imatrix` ☰
      `infill` ☰
      `llava-cli` ☰
      `lookahead` ☰
      `lookup-create` ☰
      `lookup-merge` ☰
      `lookup-stats` ☰
      `lookup` ☰
      `minicpmv-cli` ☰
      `parallel` ☰
      `passkey` ☰
      `perplexity` ☰ A tool for measuring the → https://huggingface.co/docs/transformers/perplexity[perplexity] (and other quality metrics) of a model over a given text.
      `q8dot` ☰
      `quantize` ☰
      `qwen2vl-cli` ☰
      `retrieval` ☰
      `run` ☰ A comprehensive example for running llama.cpp models. Useful for inferencing. Used with RamaLama
      `save-load-state` ☰
      `server` ☰ A lightweight, → https://github.com/openai/openai-openapi[OpenAI API] compatible, HTTP server for serving LLMs. (`llama-server -m model.gguf --port 11434`). See also `grammars/` below.
      `simple-chat` ☰
      `simple` ☰ A minimal example for implementing apps with llama.cpp. Useful for developers.
      `speculative-simple` ☰
      `speculative` ☰
      `tokenize` ☰
      `tts` ☰
      `vdot` ☰
   table }

   In addition, I also found the following executables (where the common prefix `test_` is omitted:
   table { ll
     `arg-parser` ☰
     `autorelease` ☰
     `backend-ops` ☰
     `barrier` ☰
     `c` ☰
     `chat-template` ☰
     `gguf` ☰
     `log` ☰
     `model-load-cancel` ☰
     `quantize-fns` ☰
     `quantize-perf` ☰
     `rope` ☰
     `tokenizer-0` ☰
   table }

 }

 { Using the deprecated Makefile

code {
cd llama.cpp
→ development/make -j
code }

  }

  { Trying a model

    Under → https://github.com/ggerganov/llama.cpp/tree/master/models[`models/`], I found some `*.gguf` files which I believed I could use for a first test:
code {
$ build/bin/llama-cli -m models/ggml-vocab-bert-bge.gguf --prompt "tell me a nice story" --predict 100
…
llama_model_load: error loading model: missing tensor 'token_embd.weight'
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model 'models/ggml-vocab-bert-bge.gguf'
main: error: unable to load model
code }

  Apparently, → https://github.com/ggerganov/llama.cpp/issues/3917#issuecomment-1791494136[the files under `model/` are not real models, just the vocabulary part]. The actual models need to be downloaded.

  So, I download a couple of (small) models…
code {
$ mkdir ~/llm-models
$ → tools/cURL[curl] -L https://huggingface.co/aisuko/gpt2-117M-gguf/resolve/main/ggml-model-Q4_K_M.gguf                                -o ~/llm-models/ggml-model-Q4_K_M.gguf
$ → tools/cURL[curl] -L https://huggingface.co/TheBloke/dolphin-2.2.1-mistral-7B-GGUF/resolve/main/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf -o ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf
$ (cd ~/llm-models; stat --printf '%s\t%n\n' *)
4368450304      dolphin-2.2.1-mistral-7b.Q4_K_M.gguf
112858624       ggml-model-Q4_K_M.gguf
code }
 
  … and test them with the CLI tool:
code {
$ ./build/bin/llama-cli -m ~/llm-models/ggml-model-Q4_K_M.gguf                -p "Tell me about programmers and coffee" -n 200
$ ./build/bin/llama-cli -m ~/llm-models/dolphin-2.2.1-mistral-7b.Q4_K_M.gguf  -p "Tell me about programmers and coffee" -n 200
code }

  }
  { examples/

    …

  }
  { grammars/

    GBNF (GGML BNF) as a format to constrain the output produced by llama.cpp (like, for example: only valid JSON, or emojis)

    GBNF grammars can be useed in
      • `llama-server` (where the grammer is passed in the `grammar ` body field)
      • `llama-cli` using the `--grammar` and `--grammar-file` flags
      • `llama-gbnf-validator`

    See also
      • `tests/test-json-schema-to-grammar.cpp` (to see which features are likely supported)
      • → https://github.com/ggerganov/llama.cpp/pull/5978
      • → https://github.com/ggerganov/llama.cpp/pull/6659
      • → https://github.com/ggerganov/llama.cpp/pull/6555

   `-j` (`--json-schema`) flag in action:
code {
llama-cli \
  -hfr bartowski/Phi-3-medium-128k-instruct-GGUF \
  -hff Phi-3-medium-128k-instruct-Q8_0.gguf \
  -j '{
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "name": {
                "type": "string",
                "minLength": 1,
                "maxLength": 100
            },
            "age": {
                "type": "integer",
                "minimum": 0,
                "maximum": 150
            }
        },
        "required": ["name", "age"],
        "additionalProperties": false
    },
    "minItems": 10,
    "maxItems": 100
  }' \
  -p 'Generate a {name, age}[] JSON array with famous actors of all ages.'
code }

    Any schema can be converted in the command line with
code {
examples/json_schema_to_grammar.py name-age-schema.json
code }

  }
  { scripts

    table { ll
      `build-info.sh` ☰
      `check-requirements.sh` ☰ checks all requirements files for each top-level convert*.py script.
      `ci-run.sh` ☰ What's the difference to `ci/run.sh`
      `compare-commits.sh` ☰ Checks out two different commits from the repository, builds the project and then runs `compare-llama-bench.py`
      `compare-llama-bench.py` ☰
      `debug-test.sh` ☰
      `gen-authors.sh` ☰ Adds new authors to the → https://github.com/ggerganov/llama.cpp/blob/master/AUTHORS[`AUTHORS`]
      `gen-unicode-data.py` ☰
      `get-flags.mk` ☰
      `get-hellaswag.sh` ☰
      `get_hf_chat_template.py` ☰ Fetches the Jinja chat template of a HuggingFace model.
      `get-pg.sh` ☰
      `get-wikitext-103.sh` ☰
      `get-wikitext-2.sh` ☰
      `get-winogrande.sh` ☰
      `hf.sh` ☰ Dowonload a Hugging Face model (like for example `./llama-cli -m $(./scripts/hf.sh https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf)`)
      `install-oneapi.bat` ☰
      `qnt-all.sh` ☰
      `run-all-perf.sh` ☰
      `run-all-ppl.sh` ☰
      `sync-ggml-am.sh` ☰ Synchronize ggml changes to llama.cpp
      `sync-ggml.last` ☰
      `sync-ggml.sh` ☰
      `verify-checksum-models.py` ☰
      `xxd.cmake` ☰
    table }

  }
  { cmake/x64-windows-llvm.cmake

    The `cmake` directory contains a file named `x64-windows-llvm.cmake`.

    Can this file explicitely be used for an LLVM build?

  }

}
{ TODO

  { llama build number, build commit, build compiler and build target

    → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh[`scripts/build-info.sh`] determines the following values:
    table { lll
      ~Value~ ☰ ~Command~ ☰ ~Example value~
      llama build number ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L10C10-L10C24[`git rev-list --count HEAD`] ☰ 4589
      build commit ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L15C10-L15C25[`git rev-parse --short HEAD`] ☰ eb7cf15a
      build compiler ☰ → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L19C10-L19C22[`$CC --version | head -1`] ☰ gcc (Debian 10.2.1-6) 10.2.1 20210110
      build target ☰  → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/scripts/build-info.sh#L23C10-L23C15[`$CC --dumpmachine`] ☰ x86_64-linux-gnu
    table }

    These value seems then to be used to produce `common/build-info.cpp` from → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/build-info.cpp.in[`common/build-info.cpp.in`].

    However, I don't find `build-info.sh` referenced in any other file except in the → https://github.com/ggerganov/llama.cpp/blob/27d135c970c00f655d486f870edacded792bef5c/Makefile[`Makefile`] which is
    → https://github.com/ggerganov/llama.cpp/blob/27d135c970c00f655d486f870edacded792bef5c/Makefile#L2[deprecated].

    Therefore, I now believe hat these figures are determined in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/cmake/build-info.cmake[`cmake/build-info.cmake`]
    (which in turn seems to be invoked or referenced in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/cmake/build-info-gen-cpp.cmake[`common/cmake/build-info-gen-cpp.cmake`]).

  }
  { Environment variables

    table { ll
       `LLAMA_ARG_MODEL` ☰ Model path? (for example `models/7B/ggml-model-f16.gguf`)
       `GG_BUILD_CUDA` ☰ ?
       `GG_BUILD_SYCL` ☰ ?
       `GG_BUILD_VULKAN` ☰ ?
    table }
  
  }
  { models/7B/ggml-model-f16.gguf

   `models/7B/ggml-model-f16.gguf` is the default path where llama.cpp looks for a model if not explicitely specified, see
   → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/common.h#L25[`#define DEFAULT_MODEL_PATH "models/7B/ggml-model-f16.gguf"`]
   in → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/common/common.h[`common/common.h`].

  }
  { ci/run.sh

    In addition to → https://github.com/ggerganov/llama.cpp/actions[llama.cpp's github actions], a commit to the repository triggers the execution fo → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/ci/run.sh[`ci/run.sh`].
    on dedicated cloud instances which permits heavier workloads than just Github actions.

    See → https://github.com/ggerganov/llama.cpp/blob/eb7cf15a808d4d7a71eef89cc6a9b96fe82989dc/ci/README.md[`ci/README.md`].

    What's the difference to `scripts/ci-run.sh`

  }

}
{ main

  2024-10-05: Apparently, `main` is deprecated in favor of `llama-cli`.

  { Options

    table { llll

☰ ☰ ~param value~ ☰
  `-h` ☰ `--help` ☰ ☰ Show this help message and exit
  `-i` ☰ `--interactive` ☰ ☰ Run in interactive mode
  ☰ `--interactive-first` ☰ ☰ Run in interactive mode and wait for input right away
  ☰ `-ins`, `--instruct` ☰  ☰ Run in instruction mode (use with Alpaca models)
  `-r` ☰ `--reverse-prompt` ☰ `PROMPT` ☰ Run in interactive mode and poll user input upon seeing `PROMPT` (can be specified more than once for multiple prompts).
  ☰ `--color` ☰  ☰ Colorise output to distinguish prompt and user input from generations
  `-s` ☰ `--seed` ☰ `SEED` ☰ Seed for random number generator (default: `-1`, use random seed for <= 0)
  `-t` ☰ `--threads` ☰ `N` ☰ Number of threads to use during computation (default: 12)
  `-p` ☰ `--prompt` ☰ `PROMPT` ☰ Prompt to start generation with (default: empty)
  ☰ `--random-prompt` ☰ ☰ Start with a randomized prompt.
  ☰ `--in-prefix` ☰ `STRING` ☰ String to prefix user inputs with (default: empty)
  `-f` ☰ `--file` ☰ `FNAME` ☰ Prompt file to start generation.
  `-n` ☰ `--n_predict` ☰ `N` ☰ Number of tokens to predict (default: 128, -1 = infinity)
   ☰ `--top_k` ☰ `N` ☰ Top-k sampling (default: 40)
 ☰ `--top_p` ☰ `N` ☰ Top-p sampling (default: 0.9)
 ☰ `--repeat_last_n` ☰ `N` ☰ Last n tokens to consider for penalize (default: 64)
 ☰ `--repeat_penalty` ☰ `N` ☰ Penalize repeat sequence of tokens (default: 1.1)
 `-c` ☰ `--ctx_size` ☰ `N` ☰ Size of the prompt context (default: `512`)
 ☰ `--ignore-eos` ☰ ☰ Ignore end of stream token and continue generating
 ☰ `--memory_f32` ☰ ☰ Use `f32` instead of `f16` for memory key+value
 ☰ `--temp` ☰ `N` ☰ Temperature (default: `0.8`)
  ☰ `--n_parts` ☰ `N` ☰ Number of model parts (default: -1 = determine from dimensions)
  `-b` ☰ `--batch_size` ☰ `N` ☰ Batch size for prompt processing (default: 8)
  ☰ `--perplexity` ☰ ☰ Compute perplexity over the prompt
  ☰ `--keep` ☰   ☰ Number of tokens to keep from the initial prompt (default: 0, -1 = all)
  ☰ `--mlock` ☰  ☰ Force system to keep model in RAM rather than swapping or compressing
  ☰ `--mtest` ☰  ☰ Determine the maximum memory usage needed to do inference for the given `n_batch` and `n_predict` parameters (uncomment the `"used_mem"` line in `llama.cpp` to see the results)
  ☰ `--verbose-prompt` ☰   ☰ Print prompt before generation
  `-m` ☰ `--model` ☰ `FNAME` ☰ Model path (default: `models/llama-7B/ggml-model.bin`)
    table }

  }

}

links:
  → https://github.com/ggerganov/ggml[`ggml`] is a tensor library, written in C, that is used in `llama.cpp`. In fact, the description of `ggml` reads:
  *Note that this project is under development and not ready for production use.  Some of the development is currently happening in the `llama.cpp` and → https://github.com/ggerganov/whisper.cpp[`whisper.cpp`] repos*

  → https://github.com/abetlen/llama-cpp-python[Python bindings for llama.cpp] provides
    • Low-level access to C API via `→ development/languages/Python/standard-library/ctypes`.
