$ Generative pre-trained transformers (GPT)

*Generative pre-trained transformers (GPT)* models is a family of → development/Artificial-intelligence/language-model/LLM which were → https://openai.com/research/language-unsupervised[introduced by OpenAI in 2018].

GPT is called *generative* because it generates text (on a given prompt).
-
It is *pre-trained* because it was trained on lots of text.
-
And *transformer* refers to the (decoder-only?) transformer architecture (which was introduced in 2017).

{ TODO

  Andrej Karpathy's → https://github.com/karpathy/mingpt[minGPT] and → https://github.com/karpathy/nanogpt[nanoGPT] as well as Jay Mody's → https://github.com/jaymody/picoGPT/[picoGPT] (see also → https://jaykmody.com/blog/gpt-from-scratch/[his blog post]).

}

sa:
  → development/Artificial-intelligence/language-model/LLM/GPT/2,
  → development/Artificial-intelligence/language-model/LLM/GPT/Chat



links:

  → https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf[Improving Language Understanding by Generative Pre-Training]
