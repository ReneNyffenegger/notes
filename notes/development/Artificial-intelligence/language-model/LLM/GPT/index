$ Generative pre-trained transformers (GPT)

*Generative pre-trained transformers (GPT)* models is a family of → development/Artificial-intelligence/language-model/LLM which were → https://openai.com/research/language-unsupervised[introduced by OpenAI in 2018].

GPT is called *generative* because it generates text (on a given prompt).
-
It is *pre-trained* because it was trained on lots of text.
-
And *transformer* refers to the (decoder-only?) transformer architecture (which was introduced in 2017 with the paper → development/Artificial-intelligence/history#hist-ai-attention[Attention Is All You Need]).

{ TODO

  { Relation to BERT

    Unlike BERT models, GPT models are unidirectional. 

  }
  { minGPT etc.

    Andrej Karpathy's → https://github.com/karpathy/mingpt[minGPT] and → https://github.com/karpathy/nanogpt[nanoGPT] as well as Jay Mody's → https://github.com/jaymody/picoGPT/[picoGPT] (see also → https://jaykmody.com/blog/gpt-from-scratch/[his blog post]).

    { nanoGPT

      → https://github.com/karpathy/nanoGPT, requires
        • → development/languages/Python/libraries/tqdm
        • → development/languages/Python/libraries/tiktoken
        • → development/languages/Python/libraries/datasets
        • → development/languages/Python/libraries/numpy

      When running `prepare.py` (in the directory `data/openwebtext`, Windows), I got the error message *ImportError: numpy.core.multiarray failed to import*.
      -
      I was able to solve that by running `py -m pip install numpy -I`.

    }


  }

}

sa:
  → development/Artificial-intelligence/language-model/LLM/GPT/2,
  → development/Artificial-intelligence/language-model/LLM/GPT/3,
  → development/Artificial-intelligence/language-model/LLM/GPT/Chat

links:

  → https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf[Improving Language Understanding by Generative Pre-Training]
