$ Large Language Models (LLMs)
@ LLM

{ Classification based on the Transformer architecture

  Transformer based LLMs can be clasified into the three variations:
    • Encoder only
    • Decoder only
    • Encoder-Decoder

}
{ Perplexity

  The most basic intrinsic measure of a language model's performance is its *perplexity* on a given text corpus:
  It measures how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity.

  Perplexity is related to the *cross-entropy* loss function which is used to train neural language model. 

}
{ Allocating training budget

  When training an LLM, the available budget (compute) must be allocated to
    • number of model parameters
    • number of training tokens

  Some guidance about such an allocation can be gained by applying the → https://arxiv.org/abs/2001.08361[Scaling Laws for Neural Language Models].

}


{ TODO

  ? Context size/length: the number of tokens taken into account to predict the next token.

  { Components of a transformer LLM

    A transformer LLM consists of a
      • tokenizer, a
      • stack of transformer blocks and an
      • language modeling (LM) head

  }
  { Tokens and embeddings

    The central concepts for using LLMs are tokens and embeddings.

    In a technical sense, language is a sequence of tokens.
    -
    When a model is trained, it (hopefully) captures the important token patterns that appear or emerge in the training data.

    Some tokenization algorithms include
      • Byte Pair Encoding (BPE) - typically used by GPT models
      • WordPiece (typically used by (BERT)
      • SentencePiece (FLan-T5)

    Important factors for tokenizers are
      • The size of its vocabulary (as of 2024: typically between 30 and 50K, but moving towards 100K)
      • The set of special tokens (Beginning of text (`<S>`), End of text, Padding, Unknown, classification (`[CLS]`), seperation (`[SEP]`), masking…). Models like *Galactica* which focus on scientific knowledge also include tokens like for citations, reasoning, mathematics, amino acid sequences and DNA sequences). Note that `[CLS]` and `<s>` are similar in nature.
      • How to numerically represent each token (Is this really a thing?)
      • How to deal with capitilization


    For each token in the tokenizer's vocabulary, there is an associated *embedding* which is a vector of numerical values.
    -
    The values of these vectors is initialized with random numbers before the model is trained. These values are updated as the model is trained.

  }
  { Transformer blocks

?   Each token (and in the case of autoregressive models also each generated token) is passed through the *transformer blocks*. The last tranformer block hands its output to the *LM head*.

    A transformer block basically consists of an
      • attention layer and a
      • feedforward neural network.

  }
  { LM Head

?   The LM head calculates the propability distribution for the next token.

  }
  { Decoding: Choosing next token, temperature

    After the LM head has calculated the probability distribution for the next token, the model needs to choose the next token.

    A simple strategy is to always pick the token with the highest probality score (also called *greedy token*).
    -
    The *temparature* parameter allows to define how often the decoder should deviate from that rule.
    -
    If the temparature is 0, it never deviates, the higher the temparature is, the more often it deviates.

  }
  { Context size

  ? The context size is the number of tokens that the model can operate on in parallel.

  }
  { Autoregressive models

    An *autoregressive model* predicts the next token based on based on the input plus the already predicted tokens.

    BERT is not an autoregressive model (the B stands for *bidirectional*).

  }
  { GGUF

    GGUF (GGML Universal File) is format to to stores both tensors and metadata in a single file whose goal is to save and load of model data fast.
    -
    Another focus of GGUF is quantization (which reduces memory usage and increases speed by reducing precision in the model weights, which, of course, also lowers the model's accuracy).

    GGUF was introduced as successor of file formats such as GGML in August 2023 by the llama.cpp project.

    GGUF are typically created by converting models of machine learning libraries such as PyTorch.

    { gguf-tools (antirez)

code {
git clone → https://github.com/antirez/gguf-tools
cd gguf-tools
make
curl -OL https://huggingface.co/aisuko/gpt2-117M-gguf/resolve/main/ggml-model-Q4_K_M.gguf
./gguf-tools show ggml-model-Q4_K_M.gguf
code }

    }
    { convert-hf-to-gguf.py

      → https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py[`convert_hf_to_gguf.py`] (of  → https://github.com/ggerganov/llama.cpp[llama.cpp]) converts HuggingFace to GGUF.

    }

  }
  { Claude

    { Installing Claude (Windows/PowerShell)

      Claude requires
        • a 64-bit Windows
	• `git-bash` (→ https://git-scm.com/downloads/win)

?     The latest version number can be queried from → https://storage.googleapis.com/claude-code-dist-86c565f3-f756-42ad-8dfa-d59b1c096819/claude-code-releases/latest.
      -
?     With this version number (for example `2.1.12`), a manifest file can be downloaded from → https://storage.googleapis.com/claude-code-dist-86c565f3-f756-42ad-8dfa-d59b1c096819/claude-code-releases/2.1.12/manifest.json.

      Claude can be installed with PowerShell like so: (note that the url is redirected, as of 2026-01-18 to → https://storage.googleapis.com/claude-code-dist-86c565f3-f756-42ad-8dfa-d59b1c096819/claude-code-releases/bootstrap.ps1):
code {
invoke-webrequest → https://claude.ai/install.ps1 | invoke-expression
code }

      As far as I can tell, this script essentially downloads the binary `claude.exe` and places it into `→ Windows/dirs/Users/username[C:\Users\username]\.claude\downloads\claude-$version-$platform.exe` and runs this executable to start
      the installation.

     `Claude.exe --help`
code {
Usage: claude [options] [command] [prompt]

Claude Code - starts an interactive session by default, use -p/--print for non-interactive output

Arguments:
  prompt                                            Your prompt

Options:
  --add-dir <directories...>                        Additional directories to allow tool access to
  --agent <agent>                                   Agent for the current session. Overrides the 'agent' setting.
  --agents <json>                                   JSON object defining custom agents (e.g. '{"reviewer": {"description": "Reviews code", "prompt": "You are a code reviewer"}}')
  --allow-dangerously-skip-permissions              Enable bypassing all permission checks as an option, without it being enabled by default. Recommended only for sandboxes with no internet access.
  --allowedTools, --allowed-tools <tools...>        Comma or space-separated list of tool names to allow (e.g. "Bash(git:*) Edit")
  --append-system-prompt <prompt>                   Append a system prompt to the default system prompt
  --betas <betas...>                                Beta headers to include in API requests (API key users only)
  --chrome                                          Enable Claude in Chrome integration
  -c, --continue                                    Continue the most recent conversation in the current directory
  --dangerously-skip-permissions                    Bypass all permission checks. Recommended only for sandboxes with no internet access.
  -d, --debug [filter]                              Enable debug mode with optional category filtering (e.g., "api,hooks" or "!statsig,!file")
  --disable-slash-commands                          Disable all skills
  --disallowedTools, --disallowed-tools <tools...>  Comma or space-separated list of tool names to deny (e.g. "Bash(git:*) Edit")
  --fallback-model <model>                          Enable automatic fallback to specified model when default model is overloaded (only works with --print)
  --file <specs...>                                 File resources to download at startup. Format: file_id:relative_path (e.g., --file file_abc:doc.txt file_def:img.png)
  --fork-session                                    When resuming, create a new session ID instead of reusing the original (use with --resume or --continue)
  -h, --help                                        Display help for command
  --ide                                             Automatically connect to IDE on startup if exactly one valid IDE is available
  --include-partial-messages                        Include partial message chunks as they arrive (only works with --print and --output-format=stream-json)
  --input-format <format>                           Input format (only works with --print): "text" (default), or "stream-json" (realtime streaming input) (choices: "text", "stream-json")
  --json-schema <schema>                            JSON Schema for structured output validation. Example: {"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}
  --max-budget-usd <amount>                         Maximum dollar amount to spend on API calls (only works with --print)
  --mcp-config <configs...>                         Load MCP servers from JSON files or strings (space-separated)
  --mcp-debug                                       [DEPRECATED. Use --debug instead] Enable MCP debug mode (shows MCP server errors)
  --model <model>                                   Model for the current session. Provide an alias for the latest model (e.g. 'sonnet' or 'opus') or a model's full name (e.g. 'claude-sonnet-4-5-20250929').
  --no-chrome                                       Disable Claude in Chrome integration
  --no-session-persistence                          Disable session persistence - sessions will not be saved to disk and cannot be resumed (only works with --print)
  --output-format <format>                          Output format (only works with --print): "text" (default), "json" (single result), or "stream-json" (realtime streaming) (choices: "text", "json", "stream-json")
  --permission-mode <mode>                          Permission mode to use for the session (choices: "acceptEdits", "bypassPermissions", "default", "delegate", "dontAsk", "plan")
  --plugin-dir <paths...>                           Load plugins from directories for this session only (repeatable)
  -p, --print                                       Print response and exit (useful for pipes). Note: The workspace trust dialog is skipped when Claude is run with the -p mode. Only use this flag in directories you trust.
  --replay-user-messages                            Re-emit user messages from stdin back on stdout for acknowledgment (only works with --input-format=stream-json and --output-format=stream-json)
  -r, --resume [value]                              Resume a conversation by session ID, or open interactive picker with optional search term
  --session-id <uuid>                               Use a specific session ID for the conversation (must be a valid UUID)
  --setting-sources <sources>                       Comma-separated list of setting sources to load (user, project, local).
  --settings <file-or-json>                         Path to a settings JSON file or a JSON string to load additional settings from
  --strict-mcp-config                               Only use MCP servers from --mcp-config, ignoring all other MCP configurations
  --system-prompt <prompt>                          System prompt to use for the session
  --tools <tools...>                                Specify the list of available tools from the built-in set. Use "" to disable all tools, "default" to use all tools, or specify tool names (e.g. "Bash,Edit,Read").
  --verbose                                         Override verbose mode setting from config
  -v, --version                                     Output the version number

Commands:
  doctor                                            Check the health of your Claude Code auto-updater
  install [options] [target]                        Install Claude Code native build. Use [target] to specify version (stable, latest, or specific version)
  mcp                                               Configure and manage MCP servers
  plugin                                            Manage Claude Code plugins
  setup-token                                       Set up a long-lived authentication token (requires Claude subscription)
  update                                            Check for updates and install if available
code }

    }
    { Billing method

      When claude is installed (`claude.exe install`), it asks for one of two billing methods:
        • Claude account with subscription (Pro, Max, Team or Enterprise)
	• Anthropic Console Account

    }

  }
  { Models

    A list of models is maintained by → https://lifearchitect.ai/models-table/[LifeArchitect.ai].

  { Cerebras-GPT

   *Cerebras-GPT* is a family of seven GPT models ranging from 111 million to 13 billion parameters.
   
    These models were trained on *CS-2* systems (Andromeda AI supercomputer) using the *Chinchilla formula*.
    -
    Their weights and checkpoints are available on Hugging Face and GitHub under the Apache 2.0 license.

  }
  { GPT-4

    GPT-4 was released without any information about its model architecture, training data, training hardware or hyperparameters.

    As per → https://www.semafor.com/article/03/24/2023/the-secret-history-of-elon-musk-sam-altman-and-openai[The secret history of Elon Musk, Sam Altman, and OpenAI], GPT-4 has 1 trillion parameters.

  }
  { Chinchilla (DeepMind)

    Same budget as Gopher but with 70 B parameters and 4 times more data.

    Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3
    (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range
    of downstream evaluation tasks. It uses substantially less computing for
    fine-tuning and inference, greatly facilitating downstream usage.

  }
  { Gemini

    Gemini is Google's → https://youtu.be/u_dSUtp4eM8?t=855[next generation foundation model] (as of 2023-05-13):
   "
     This model is created from the ground up to be multi modal, highly efficient at tool and API integrations and
     built to enable future innovations like memory and planning.
   "

  }

  }
  { Scaling laws

     J. Kaplan, S. McCandlish: → https://arxiv.org/pdf/2001.08361.pdf[Scaling Laws for Neural Language Models]:
     "
        We study empirical scaling laws for language model performance on the cross-entropy loss.
     "

  }
  { Benchmarks

    { BIG-Bench

      BIG-Bench is a collaborative benchmark with over 150 tasks which aim at producing challenging tasks for LLMs.

      The tasks include
        • logical reasoning,
        • translation,
        • question answering,
        • mathematics, and
        • others

    }

  }
  { Finding and downloading LLMs / PTMs

    Many LLMs and/or PTMs are exchanged on registries (quite similar to how software components are shared in → development/languages/JavaScript/npm[NPM], → development/languages/Python/index#py-pypi[PyPI], Maven etc.)

    One of these registries is the → https://huggingface.co/docs/hub/en/index[Hugging Face Hub].
    -
    As of 2024-10, this hub hosts 900k models, 200k datasets and 300k demos.

  }

}

sa:

  → development/Artificial-intelligence/language-model

  → development/Artificial-intelligence/language-model/LLM/LLaMA

  Oracle's package `→ development/databases/Oracle/installed/packages/dbms/cloud_ai` uses LLMs to generate → development/databases/Oracle/SQL/statement[SQL statements] from natural language prompts.
  -
  But see also the `→ development/databases/Oracle/SQL/select/ai` clause.
