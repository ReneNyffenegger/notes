$ Large Language Models
-

{ Allocating training budget

  When training an LLM, the available budget (compute) must be allocated to
    • number of model parameters
    • number of training tokens

  Some guidance about such an allocation can be gained by applying the → https://arxiv.org/abs/2001.08361[Scaling Laws for Neural Language Models].

}

{ TODO

  ? Context size/length: the number of tokens taken into account to predict the next token.

}

sa:
  → development/Artificial-intelligence/language-model/LLM/LLaMA
