$ Large Language Models

{ Allocating training budget

  When training an LLM, the available budget (compute) must be allocated to
    • number of model parameters
    • number of training tokens

  Some guidance about such an allocation can be gained by applying the → https://arxiv.org/abs/2001.08361[Scaling Laws for Neural Language Models].

}

{ TODO

  ? Context size/length: the number of tokens taken into account to predict the next token.

  { Models

    A list of models is maintained by → https://lifearchitect.ai/models-table/[LifeArchitect.ai].

  { Cerebras-GPT

   *Cerebras-GPT* is a family of seven GPT models ranging from 111 million to 13 billion parameters.
   
    These models were trained on *CS-2* systems (Andromeda AI supercomputer) using the *Chinchilla formula*.
    -
    Their weights and checkpoints are available on Hugging Face and GitHub under the Apache 2.0 license.

  }
  { GPT-4

    GPT-4 was released without any information about its model architecture, training data, training hardware or hyperparameters.

    As per → https://www.semafor.com/article/03/24/2023/the-secret-history-of-elon-musk-sam-altman-and-openai[The secret history of Elon Musk, Sam Altman, and OpenAI], GPT-4 has 1 trillion parameters.

  }
  { Chinchilla (DeepMind)

    Same budget as Gopher but with 70 B parameters and 4 times more data.

    Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3
    (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range
    of downstream evaluation tasks. It uses substantially less computing for
    fine-tuning and inference, greatly facilitating downstream usage.

  }

  }
  { Scaling laws

     J. Kaplan, S. McCandlish: → https://arxiv.org/pdf/2001.08361.pdf[Scaling Laws for Neural Language Models]:
     "
        We study empirical scaling laws for language model performance on the cross-entropy loss.
     "

  }

}

sa:
  → development/Artificial-intelligence/language-model/LLM/LLaMA
