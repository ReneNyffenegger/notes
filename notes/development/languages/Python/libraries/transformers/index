$ Python library: transformers
@ transformers

code {
pip install transformers
code }

code {
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(       "EleutherAI/gpt-j-6B")
model     = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")
code }

{ Standard classes

? A model is required to use three classes derived from
   • `PreTrainedModel`
   • `PreTrainedConfig`
   • `PreTrainedTokenizer` (NLP), `ImageProcessingMixin` (?) for vision, `FeatureExtractionMixin` (?) for audio, `ProcessorMixin` (?) for multimodal inputs

  Instances of these classes are created by calling `from_pretrained()`. This method downloads (if necessary), caches and loads the followin related data from a pretrained checkpoint provided on Hugging Face Hub/
     • Hyperparameters of configuration
     • Vocabulary of tokenizer
     • Weights of the model

}

{ «Base» classes

  { PreTrainedModel #py-transformers-PreTrainedModel

   `PreTrainedModel` is the base class for all models and implements the  methods for loading and saving a model.

   `PreTrainedModel` immediatly derives from:
code {
>>> import transformers
>>> for parent in transformers.PreTrainedModel.→ development/languages/Python/dunders/__bases__:
...     print(f'{parent.__name__:<20} {parent.__module__}')
...
→ development/languages/Python/libraries/torch/types/nn/Module[Module]               torch.nn.modules.module
ModuleUtilsMixin     transformers.modeling_utils
GenerationMixin      transformers.generation.utils
PushToHubMixin       transformers.utils.hub
code }

  }
  { TFPreTrainedModel

    Same thing, but for `TFPreTrainedModel`:
code {
>>> → development/languages/Python/statements/import transformers
>>> for parent in transformers.TFPreTrainedModel.→ development/languages/Python/dunders/__bases__:
...     print(f'{parent.__name__:<20} {parent.__module__}')
...
Model                → development/languages/Python/libraries/TensorFlow/Keras[keras].engine.training
TFModelUtilsMixin    transformers.modeling_tf_utils
TFGenerationMixin    transformers.generation.tf_utils
PushToHubMixin       transformers.utils.hub
code }

  }
  { FlaxPreTrainedModel

    Same thing, but for `FlaxPreTrainedModel`:
code {
>>> → development/languages/Python/statements/import transformers
>>> for parent in transformers.FlaxPreTrainedModel.→ development/languages/Python/dunders/__bases__:
...     print(f'{parent.__name__:<20} {parent.__module__}')
...
object               builtins
code }

  }


}
{ PreTrainedConfig

 `PreTrainedConfig` is the base class that implements methods for loading and saving configurations.

  Common attributes in derived classes include
    • `hidden_size`
    • `num_attention_heads`
    • `num_hidden_layers`
    • `vocab_size` (Text models only)

}
{ GPT2LMHeadModel

 `GPT2LMHeadModel` inherits from `GPT2PreTrainedModel` which inherits from → #py-transformers-PreTrainedModel[`PreTrainedModel`]:
code {
>>> → development/languages/Python/statements/import transformers
>>> for baseClass in transformers.GPT2LMHeadModel.__bases__:
...     print(f'{baseClass.__name__:<20} {baseClass.__module__}')
...
GPT2PreTrainedModel  transformers.models.gpt2.modeling_gpt

>>> for baseClass in transformers.GPT2PreTrainedModel.__bases__:
...     print(f'{baseClass.__name__:<20} {baseClass.__module__}')
...
→ #py-transformers-PreTrainedModel[PreTrainedModel]      transformers.modeling_utils
code }

}
{ from_pretrained()

  …

}
