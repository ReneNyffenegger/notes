$ Python: torch.Tensor

{ Members

  Use → development/languages/Python/built-in-functions/dir[`dir()`] and → development/languages/Python/types/list/comprehension to find the members of the `Tensor` class of
  → development/languages/Python/libraries/torch that don't start with two underscores (i.e. exclude dunders) and don't end in an underscore:
code {
for member in → development/languages/Python/built-in-functions/sorted[sorted]([ _ for _ in dir(torch.Tensor) if  len(_) > 1 and _[1] != '_'  and _[-1] != '_'], key = → development/languages/Python/expressions/lambda[lambda] s: s.→ development/languages/Python/types/string/replace[replace]('_', '')):
    → development/languages/Python/built-in-functions/print[print](member)
code }

  table { ll

    `abs` ☰
    `absolute` ☰
    `acos` ☰
    `acosh` ☰
    `add` ☰
    `addbmm` ☰
    `addcdiv` ☰
    `addcmul` ☰
    `addmm` ☰
    `_addmm_activation` ☰
    `addmv` ☰
    `addr` ☰
    `adjoint` ☰
    `align_as` ☰
    `align_to` ☰
    `all` ☰ Tests if all elements in the tensor are `True` (and returns the result as a tensor with one boolean element?). Compare with `any()`
    `allclose` ☰
    `amax` ☰
    `amin` ☰
    `aminmax` ☰
    `angle` ☰
    `any` ☰ Testsi if at least one element in the tensor is `True`. Compare with `all()`.
    `arccos` ☰
    `arccosh` ☰
    `arcsin` ☰
    `arcsinh` ☰
    `arctan` ☰
    `arctan2` ☰
    `arctanh` ☰
    `argmax` ☰
    `argmin` ☰
    `argsort` ☰
    `argwhere` ☰
    `asin` ☰
    `asinh` ☰
    `as_strided` ☰
    `as_strided_scatter` ☰
    `as_subclass` ☰
    `atan` ☰
    `atan2` ☰
    `atanh` ☰
    `_autocast_to_full_precision` ☰
    `_autocast_to_reduced_precision` ☰
    `backward` ☰ Computes the gradients of the current tensor (see also `grad` and `requires_grad`).
    `_backward_hooks` ☰
    `baddbmm` ☰
    `_base` ☰
    `bernoulli` ☰
    `bfloat16` ☰
    `bincount` ☰
    `bitwise_and` ☰
    `bitwise_left_shift` ☰
    `bitwise_not` ☰
    `bitwise_or` ☰
    `bitwise_right_shift` ☰
    `bitwise_xor` ☰
    `bmm` ☰
    `bool` ☰
    `broadcast_to` ☰
    `byte` ☰
    `ccol_indices` ☰
    `_cdata` ☰
    `cdouble` ☰
    `ceil` ☰
    `cfloat` ☰
    `chalf` ☰
    `char` ☰
    `cholesky` ☰ Deprecated in favor of `torch.linalg.cholesky()` (will be removed)
    `cholesky_inverse` ☰
    `cholesky_solve` ☰
    `chunk` ☰
    `clamp` ☰
    `clamp_max` ☰
    `clamp_min` ☰
    `clip` ☰
    `clone` ☰
    `coalesce` ☰
    `col_indices` ☰
    `_conj` ☰
    `conj` ☰
    `_conj_physical` ☰
    `conj_physical` ☰
    `contiguous` ☰
    `copysign` ☰
    `corrcoef` ☰
    `cos` ☰
    `cosh` ☰
    `count_nonzero` ☰
    `cov` ☰
    `cpu` ☰  Returns a copy of this object in CPU memory. Compare with `cuda`
    `cross` ☰
    `crow_indices` ☰
    `cuda` ☰ Returns a copy of this object in CUDA memory. Compare with `cpu`
    `cummax` ☰
    `cummin` ☰
    `cumprod` ☰
    `cumsum` ☰
    `data` ☰ → https://stackoverflow.com/a/51744091[deprecated]
    `data_ptr` ☰ The address of the first element.
    `deg2rad` ☰
    `dense_dim` ☰
    `dequantize` ☰
    `det` ☰
    `detach` ☰
    `device` ☰ the `torch.device` where the tensor is located.
    `diag` ☰
    `diag_embed` ☰
    `diagflat` ☰
    `diagonal` ☰
    `diagonal_scatter` ☰
    `diff` ☰
    `digamma` ☰
    `dim` ☰ The number of dimensions of the tensor. An alias is `ndim`
    `_dimI` ☰
    `_dimV` ☰
    `dist` ☰
    `div` ☰
    `divide` ☰
    `dot` ☰
    `double` ☰
    `dsplit` ☰
    `dtype` ☰
    `eig` ☰
    `element_size` ☰
    `eq` ☰
    `equal` ☰
    `erf` ☰
    `erfc` ☰
    `erfinv` ☰
    `exp` ☰
    `exp2` ☰
    `expand` ☰
    `expand_as` ☰
    `expm1` ☰
    `fix` ☰
    `_fix_weakref` ☰
    `flatten` ☰
    `flip` ☰
    `fliplr` ☰
    `flipud` ☰
    `float` ☰
    `float_power` ☰
    `floor` ☰
    `floor_divide` ☰
    `fmax` ☰
    `fmin` ☰
    `fmod` ☰
    `frac` ☰
    `frexp` ☰
    `gather` ☰
    `gcd` ☰
    `ge` ☰
    `geqrf` ☰
    `ger` ☰
    `get_device` ☰
    `_grad` ☰
    `grad` ☰ This attribute becomes a tensor when `backward()` is called to compute the gradients (for self). See also `requires_grad`.
    `_grad_fn` ☰
    `grad_fn` ☰
    `greater` ☰
    `greater_equal` ☰
    `gt` ☰
    `half` ☰
    `hardshrink` ☰
    `has_names` ☰
    `_has_symbolic_sizes_strides` ☰
    `heaviside` ☰
    `histc` ☰
    `histogram` ☰
    `hsplit` ☰
    `hypot` ☰
    `i0` ☰
    `igamma` ☰
    `igammac` ☰
    `imag` ☰
    `index_add` ☰
    `index_copy` ☰
    `index_fill` ☰
    `index_put` ☰
    `index_reduce` ☰
    `index_select` ☰
    `_indices` ☰
    `indices` ☰
    `inner` ☰
    `int` ☰
    `int_repr` ☰
    `inverse` ☰
    `ipu` ☰
    `isclose` ☰
    `is_coalesced` ☰
    `is_complex` ☰
    `is_conj` ☰
    `is_contiguous` ☰
    `is_cpu` ☰
    `is_cuda` ☰
    `is_distributed` ☰
    `isfinite` ☰
    `is_floating_point` ☰
    `isinf` ☰
    `is_inference` ☰
    `is_ipu` ☰
    `is_leaf` ☰ `False`, by default, for all tensors that with `requires_grad` being `False`.
    `is_meta` ☰
    `is_mkldnn` ☰
    `is_mps` ☰
    `isnan` ☰
    `is_neg` ☰
    `isneginf` ☰
    `is_nested` ☰
    `is_nonzero` ☰
    `is_ort` ☰
    `is_pinned` ☰
    `isposinf` ☰
    `is_quantized` ☰
    `isreal` ☰
    `is_same_size` ☰
    `is_set_to` ☰
    `is_shared` ☰
    `is_signed` ☰
    `is_sparse` ☰
    `is_sparse_csr` ☰
    `istft` ☰
    `_is_view` ☰
    `is_vulkan` ☰
    `is_xpu` ☰
    `_is_zerotensor` ☰
    `item` ☰ Compare with `tolist()`
    `kron` ☰
    `kthvalue` ☰
    `layout` ☰
    `lcm` ☰
    `ldexp` ☰
    `le` ☰
    `lerp` ☰
    `less` ☰
    `less_equal` ☰
    `lgamma` ☰
    `log` ☰
    `log10` ☰
    `log1p` ☰
    `log2` ☰
    `logaddexp` ☰
    `logaddexp2` ☰
    `logcumsumexp` ☰
    `logdet` ☰
    `logical_and` ☰
    `logical_not` ☰
    `logical_or` ☰
    `logical_xor` ☰
    `logit` ☰
    `log_softmax` ☰
    `logsumexp` ☰
    `long` ☰
    `lstsq` ☰
    `lt` ☰
    `lu` ☰
    `lu_solve` ☰
    `mH` ☰
    `mT` ☰
    `_make_subclass` ☰
    `_make_wrapper_subclass` ☰
    `masked_fill` ☰
    `masked_scatter` ☰
    `masked_select` ☰
    `matmul` ☰
    `matrix_exp` ☰
    `matrix_power` ☰
    `max` ☰
    `maximum` ☰
    `mean` ☰
    `median` ☰
    `min` ☰
    `minimum` ☰
    `mm` ☰
    `mode` ☰
    `moveaxis` ☰
    `movedim` ☰
    `msort` ☰
    `mul` ☰
    `multinomial` ☰
    `multiply` ☰
    `mv` ☰
    `mvlgamma` ☰
    `name` ☰
    `names` ☰
    `nanmean` ☰
    `nanmedian` ☰
    `nanquantile` ☰
    `nansum` ☰
    `nan_to_num` ☰
    `narrow` ☰
    `narrow_copy` ☰
    `ndim` ☰
    `ndimension` ☰
    `ne` ☰
    `neg` ☰
    `negative` ☰
    `_neg_view` ☰
    `nelement` ☰
    `_nested_tensor_layer_norm` ☰
    `_nested_tensor_size` ☰
    `new` ☰
    `new_empty` ☰
    `new_empty_strided` ☰
    `new_full` ☰
    `new_ones` ☰
    `new_tensor` ☰
    `new_zeros` ☰
    `nextafter` ☰
    `_nnz` ☰
    `nonzero` ☰
    `norm` ☰
    `not_equal` ☰
    `numel` ☰
    `numpy` ☰
    `orgqr` ☰
    `ormqr` ☰
    `outer` ☰
    `output_nr` ☰
    `permute` ☰
    `pin_memory` ☰
    `pinverse` ☰
    `polygamma` ☰
    `positive` ☰
    `pow` ☰
    `prelu` ☰
    `prod` ☰
    `put` ☰
    `_python_dispatch` ☰
    `qr` ☰
    `qscheme` ☰
    `quantile` ☰
    `rad2deg` ☰
    `ravel` ☰
    `real` ☰
    `reciprocal` ☰
    `record_stream` ☰
    `_reduce_ex_internal` ☰
    `refine_names` ☰
    `register_hook` ☰
    `reinforce` ☰
    `relu` ☰
    `remainder` ☰
    `rename` ☰
    `renorm` ☰
    `repeat` ☰
    `repeat_interleave` ☰
    `requires_grad` ☰ This tensor's gradient need to be computed if set to `True`. (See also `backward` and `grad`)
    `reshape` ☰
    `reshape_as` ☰
    `resize` ☰
    `resize_as` ☰
    `resolve_conj` ☰
    `resolve_neg` ☰
    `retain_grad` ☰
    `retains_grad` ☰
    `roll` ☰
    `rot90` ☰
    `round` ☰
    `row_indices` ☰
    `rsqrt` ☰
    `scatter` ☰
    `scatter_add` ☰
    `scatter_reduce` ☰
    `select` ☰
    `select_scatter` ☰
    `sgn` ☰
    `shape` ☰
    `short` ☰
    `sigmoid` ☰
    `sign` ☰
    `signbit` ☰
    `sin` ☰
    `sinc` ☰
    `sinh` ☰
    `size` ☰
    `slice_scatter` ☰
    `slogdet` ☰
    `smm` ☰
    `softmax` ☰
    `solve` ☰
    `sort` ☰
    `sparse_dim` ☰
    `sparse_mask` ☰
    `split` ☰
    `split_with_sizes` ☰
    `sqrt` ☰
    `square` ☰
    `squeeze` ☰
    `sspaddmm` ☰
    `std` ☰
    `stft` ☰
    `_storage` ☰
    `storage` ☰
    `storage_offset` ☰
    `storage_type` ☰
    `stride` ☰
    `sub` ☰
    `subtract` ☰
    `sum` ☰
    `sum_to_size` ☰
    `svd` ☰
    `swapaxes` ☰
    `swapdims` ☰
    `symeig` ☰
    `take` ☰
    `take_along_dim` ☰
    `tan` ☰
    `tanh` ☰
    `tensor_split` ☰
    `tile` ☰
    `to` ☰ Perform *dtype* and/or *device* conversion.
    `_to_dense` ☰
    `to_dense` ☰
    `tolist` ☰ Returns the tensor's items as a (potentially nested) → development/languages/Python/types/list. Compare with `item()`
    `to_mkldnn` ☰
    `to_padded_tensor` ☰
    `topk` ☰
    `to_sparse` ☰
    `to_sparse_bsc` ☰
    `to_sparse_bsr` ☰
    `to_sparse_coo` ☰
    `to_sparse_csc` ☰
    `to_sparse_csr` ☰
    `trace` ☰
    `transpose` ☰
    `triangular_solve` ☰
    `tril` ☰
    `triu` ☰
    `true_divide` ☰
    `trunc` ☰
    `type` ☰ Returns the type if *dtype* is not provided, or cast the object to the given type.
    `type_as` ☰
    `unbind` ☰
    `unflatten` ☰
    `unfold` ☰
    `unique` ☰
    `unique_consecutive` ☰
    `unsafe_chunk` ☰
    `unsafe_split` ☰
    `unsafe_split_with_sizes` ☰
    `unsqueeze` ☰
    `_update_names` ☰
    `_values` ☰
    `values` ☰
    `var` ☰
    `vdot` ☰
    `_version` ☰
    `view` ☰
    `view_as` ☰
    `volatile` ☰
    `vsplit` ☰
    `where` ☰
    `xlogy` ☰
    `xpu` ☰
    `zero_` (?) ☰ Fills tensor with zeros.
  table }

}
