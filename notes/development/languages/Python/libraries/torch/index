$ Python library: torch (PyTorch)

{ Multiplication

code {
>>> a = torch.tensor([ 2, 4, 3 ])
>>> a * 3
tensor([ 6, 12,  9])

>>> b = torch.tensor([ 5, 7, 2 ])
>>> torch.mul(a, b)
tensor([10, 28,  6])

>>> a * b
tensor([10, 28,  6])
code }

code {
>>> a = torch.tensor([[ 1, 2, 3, 4 ]])
>>> a
tensor([[1, 2, 3, 4]])

>>> b = torch.tensor([ [40], [30], [20], [10] ])
>>> b
tensor([[40],
        [30],
        [20],
        [10]])

>>> a*b
tensor([[ 40,  80, 120, 160],
        [ 30,  60,  90, 120],
        [ 20,  40,  60,  80],
        [ 10,  20,  30,  40]])
code }

}
{ Data types (dtype)

  Although → development/languages/Python does not distinguish between floats and doubles (they're called floats, but are double precision), In torch, there is such a distinction:
code {
>>> i = torch.tensor([ 1  , 2  , 3   ]                    ) ; i.dtype
torch.int64

>>> f = torch.tensor([ 1.1, 2.2, 3.3 ]                    ) ; f.dtype
torch.float32

>>> d = torch.tensor([ 1.1, 2.2, 3.3 ], dtype=torch.double) ; d.dtype
torch.float64
code }

}
{ item() / tolist()

 `tensor.item()` obtains the numerical value as a standard Python number if the tensor has *one element only*:
code {
>>> import torch
>>> t = torch.tensor([ 4.2 ])
>>> t.item()
4.199999809265137
code }

  If the tensor has more than one element, `item()` throws a `ValueError` exception:
code {
>>> tt = torch.tensor([ 4.2, 5.3, 6.1 ])
>>> tt.item()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: only one element tensors can be converted to Python scalars
code }

 `tensor.tolist()` returns the elements of the tensors as a → development/languages/Python/types/list:
code {
>>> tt.tolist()
[4.199999809265137, 5.300000190734863, 6.099999904632568]
code }

}
{ pow(2).sum().item()

  The sequence `pow(2).sum().item()` might be used to calculate a *loss*:
code {
>>> import torch

>>> y        = torch.tensor([ 2.0, 5.0,  8.0 ])
>>> expected = torch.tensor([ 3.0, 3.0, 11.0 ])

>>> (y-expected)
tensor([-1.,  2., -3.])

>>> (y-expected).pow(2)
tensor([1., 4., 9.])

>>> (y-expected).pow(2).sum()
tensor([14.])

>>> loss = (y-expected).pow(2).sum().item()
>>> loss.item()
14.0
code }

}
{ data.item(), backward(), grad

  Import torch and create a few simple tensors:
code {
import torch

x = torch.Tensor([3.0]).double() ; x.requires_grad = True
a = torch.Tensor([4.0]).double() ; a.requires_grad = True
b = torch.Tensor([2.0]).double() ; b.requires_grad = True
c = torch.Tensor([5.0]).double() ; c.requires_grad = True
code }

  Let `y` be dependent on `a`, `b`, `c` and `x`:
code {
y = a * x**2  +  b * x  +  c
code }

  The following prints `47.0` (= 4*3² + 2*3 + 5)
code {
print(y.item())
code }

  With `y.backward()`, it's possible to calculate the gradients of the variables that have an influence on the value of `y`.
code {
y.backward()
code }

  The following prints 26 (= 2ax + b)
code {
print(x.grad.item())
code }

  The following prints 9 (=x³):
code {
print(a.grad.item())
code }

}
{ device

code {
dev   = torch.device("cpu")
# dev = torch.device("cuda:0") # Uncomment  to run on GPU
…
rnd = torch.randn((), device=dev, dtype=torch.float)
code }

}

sa:
  The → development/languages/Python/libraries/torch/types/Tensor[`Tensor`] class.
