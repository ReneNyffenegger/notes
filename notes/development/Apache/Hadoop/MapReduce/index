$ Hadoop MapReduce

Hadoop MapReduce is a framework for processing large datasets in parallel using
a → development/Apache/Hadoop/cluster[cluster] of lots of computers.
-
It behaves as if were one big computer.

? Hadoop's MapReduce is a disk based two-stage paradigm. → development/Apache/Spark[Apache Spark], in contrast, uses an in-memory primitive.

{ Architcture

~batch-based~: it is therefore weak when dealing with *real-time* data.

~shared nothing~: might be problematic for some algorithms

}
sa:
  → science/computer/Programming-paradigm/Functional-programming/MapReduce
