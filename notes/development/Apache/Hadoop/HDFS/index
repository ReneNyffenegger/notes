HDFS is designed to solve the → Linux/filesystem/NFS#limitations[limitations of NFS]. It can
store large amounts of data. It is fault tolerant (if a machine in the cluster fails, it causes
no disruption of service). If too many clients try to access the data resulting in a performance
bottleneck, adding more nodes to the cluster remedy it. And finally, HDFS integrates well
with → development/Apache/Hadoop/MapReduce.

HDFS is modelled after GFS (Google Files System).

{ Strength

  Files are optimzed for high-throughput sequential reads and writes over large files.

}
{ Weaknesses

  Inefficient for small files (but the goal was to handle big files anyway).

  Lack for (transparent) compression.

  No random access to files: data can only appended to files.

}
