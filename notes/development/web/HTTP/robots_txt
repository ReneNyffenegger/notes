$ robots.txt

{ Special characters

 `#` starts a comment that spans to the end of the line

 `*` zero or more characters (including `/`)

 `$` specifies a *hard* end of the match pattern

}
{ Example

code {
User-agent: facebookexternalhit
User-agent: Twitterbot
Allow: /foo
Allow: /bar
Allow: /*/shop/*/offers$
Disallow: /baz

Sitemap: https://server.xyz/sitemap.xml
code }

}
{ crawl-delay

 `crawl-delay` is an unofficial directive for crawlers.

  This directive is honored, for example, by Common Crawl, but ignored by Google or Yandex.

}

sa:

  → development/web/RFC#rfc-9303[RFC 9309]

  → development/web/HTTP/request#http-req-known[«Known» HTTP request paths]

  The Python standard library class `urllib.robotparser.RobotFileParser`

  → development/web/crawling

 `ads.txt`

  { humans.txt

   Some websites also have a `humans.txt` file (such as for example → https://www.init7.net/humans.txt[init7.net]).

  }

links:
 → http://www.robotstxt.org/[The Web Robots Pages]: → http://www.robotstxt.org/robotstxt.html[about robots.txt]

 The → https://github.com/google/robotstxt[robotstxt github repository] contains → Companies-Products/Google[Google's] robots.txt parser and matcher as a C++ library
