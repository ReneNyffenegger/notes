$ robots.txt

{ Special characters

 `#` starts a comment that spans to the end of the line

 `*` zero or more characterss

 `$` speifies a *hard* end of the match pattern

}
{ Example

code {
User-agent: facebookexternalhit
User-agent: Twitterbot
Allow: /foo
Allow: /bar
Allow: /*/shop/*/offers$
Disallow: /baz

Sitemap: https://server.xyz/sitemap.xml
code }

}
{ crawl-delay

 `crawl-delay` is an unofficial directive for crawlers.

  This directive is honored, for example, by Common Crawl, but ignored by Google or Yandex.

}

sa:
  → development/web/HTTP/request#http-req-known[«Known» HTTP request paths]

links:
 → http://www.robotstxt.org/[The Web Robots Pages]: → http://www.robotstxt.org/robotstxt.html[about robots.txt]

 The → https://github.com/google/robotstxt[robotstxt github repository] contains → Companies-Products/Google[Google's] robots.txt parser and matcher as a C++ library
