
{ Wikimedia projects

  TODO: Is this a (complete) list of Wikimedia projects?
  -
  Compare Wikimedia projects with → https://en.wikipedia.org/wiki/Wikipedia:WikiProject[(Wikipedia) WikiProjects].

  table { ll
    Wikipedia ☰ All the world's knowledge
    → development/Data/open/Wikidata ☰ The database of structured information
    → development/Data/open/Wikimedia/Commons ☰ The world’s largest free-to-use-library of illustrations, photos, drawings, videos and music.
    Wikibooks ☰ Collection of free e-book resources (textboosk, annotated texts, instructional guides, manuals)
    Wiktionary ☰ A dictionary for over 170 languages which aims to describe all words of all languages
    Wikiquote ☰ A collection of sourced quotations from notable people and creative works in 75 languages
    Wikisource ☰
    Wikiversity ☰
    Wikivoyage ☰
    Wikispecies ☰ A taxonomy-database that covers living and fossil representatives of Animalia, Plantae, Fungi, Bacteria, Archaea, Protista etc. The corresponsing url seems to be `→ https://species.wikimedia.org/[species.wikimedia.org]/‥`!
    Wikistats ☰ → https://stats.wikimedia.org/[stats.wikimedia.org]
    Wikitech ☰ Technical documentation of the Wikimedia Foundation infrastructure and services. → https://wikitech.wikimedia.org/wiki/Main_Page[wikitech.wikimedia.org]
    Wiki Replicas ☰
    Wikimedia Cloud Services (WMCS) ☰ Tools, services and support for technical collaborators of Wikimeda software projects.

  table }

  See also → https://opendata.renenyffenegger.ch/Wikimedia/Wikidata/entity/Q14827288[Q14827288].

}
{ Misc

  { Quarry

    Quarry is a public querying interface for Wiki Replicas and is designed
    to make running queries against Wiki Replicas easy.

    → https://quarry.wmcloud.org/

 }
 { PAWS

   PAWS is a → development/Notebook-interface/Jupyter notebook installation hosted by Wikimedia. PAWS
   notebooks can be used for creating tutorials, running live code,
   creating data visualizations, running bots using Pywikibot, and more.

  }
  { Wikimedia Foundation

   The (nonprofit) Wikimedia Foundation (WMF) hosts *Wikimedia projects'* websites (Wikipedia, Wikinews etc:)

  }
  { Wikimedia Analytics API

    The Wikimedia Analytics API allows to access data aobut Wikimedia projects such as
      • page views
      • unique devices
      • etc.

    The API endpoint is `https://wikimedia.org/api/rest_v{version number}/metrics/‥`, for example
      • → https://wikimedia.org/api/rest_v1/metrics/pageviews/top/de.wikipedia/all-access/2025/09/22
      • → https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/en.wikipedia.org/all-access/all-agents/monthly/2023010100/2024010100

    The following example uses `→ development/languages/Python/libraries/requests` to download the top 1000 views articles of a given day on a given wikipedia project:
code {
#!/usr/bin/env python3

import requests

project = 'en.wikipedia'
access  = 'all-access'
year    = '2025'
month   = '09'
day     = '22'


res = requests.get(
  f'https://wikimedia.org/api/rest_v1/metrics/pageviews/top/{project}/{access}/{year}/{month}/{day}',
#   headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }
    headers = {'User-Agent': "Rene's Wikimedia API test" }
)

if res.status_code != 200:
 #
 # Alternatively, use res.raise_for_status()
 #
   print(f"HTTP Status: {res.status_code}")
   quit()

data = res.json()

for i in data['items']:
  # print(i['project'])
  # print(i['access' ])
  # print(i['year'   ])
  # print(i['month'  ])
  # print(i['day'    ])

    for a in i['articles']:
       print(f"{a['rank']:>4} {a['views']:>5} {a['article']}")
code }

    See also: → https://doc.wikimedia.org/generated-data-platform/aqs/analytics-api/

    { Analytics Query Service (AQS)

      → https://wikitech.wikimedia.org/wiki/Data_Platform/Systems/AQS

    }
  }

}
{ Dumps

  Wikimedia Dumps can be be downloaded from → https://dumps.wikimedia.org/[dumps.wikimedia.org].

  { Pageviews and pageviews complete

    There is
      • → https://dumps.wikimedia.org/other/pageviews/[pageviews] and
      • → https://dumps.wikimedia.org/other/pageview_complete/[pageviews complete]

    { Pageviews

      Pageviews includes hourly aggregated data, such as → https://dumps.wikimedia.org/other/pageviews/2025/2025-10/projectviews-20251031-130000.gz[projectviews-20251031-130000.gz].
      -
      The format of en extracted file is so wikipedia language, article, pageviews and a final `0`:
code {
…
en 1754_in_Ireland 1 0
en 1754_in_Scotland 1 0
en 1754_in_Wales 1 0
en 1754_in_literature 1 0
en 1755_Lisbon_earthquake 10 0
en 1756_in_Denmark 1 0
en 1756_in_archaeology 1 0
en 1757_Hajj_caravan_raid 1 0
en 1757_raid_on_Berlin 1 0
en 1758_in_literature 1 0
en 175_Park_Avenue 5 0
…
code }

      Interestingly, *pageviews* also contains «projectviews», for example → https://dumps.wikimedia.org/other/pageviews/2025/2025-10/projectviews-20251002-170000.gz[projectviews-20251002-170000.gz].

    }
    { pageviews complate

      The *pageviews complete* links goes to daily aggregated data with a `pageviews-YYYYMMDD-automated.bz2` and `pageviews-YYYYMMDD-user.bz2` file.

      The format of the uncompressed `pageviews-YYYYMMDD-user` file is
code {
…
en.wikipedia 1755_(band) 28386106 mobile-web 2 B1W1
en.wikipedia 1755_(band) 28386106 desktop 4 F2L1N1
en.wikipedia 1755_(band) null mobile-app 1 C1
en.wikipedia 1755_Cape_Ann_earthquake null mobile-app 1 A1
en.wikipedia 1755_Cape_Ann_earthquake 23516172 desktop 10 H1O1Q2R2S2U1W1
en.wikipedia 1755_Cape_Ann_earthquake 23516172 mobile-web 8 A1D1H1I1K1R1T2
en.wikipedia 1755_Lisbon_earthquake 352339 desktop 412 A10B13C13D13E15F4G7H2I13J14K14L15M21N17O26P39Q29R29S18T16U23V14W28X19
en.wikipedia 1755_Lisbon_earthquake null mobile-app 39 A2B1D1E3F1G4I1K1M1P1Q2R2T5U2V5W1X6
en.wikipedia 1755_Lisbon_earthquake 352339 mobile-web 604 A26B20C16D14E4F7G14H15I25J20K26L25M26N27O29P29Q28R41S38T30U24V41W40X39
en.wikipedia 1755_Meknes_earthquake 72397816 mobile-web 5 C1F1I1M1W1
en.wikipedia 1755_Meknes_earthquake 72397816 desktop 4 J1M1X2
…
code }

      The format of the uncompressed `pageviews-YYYYMMDD-automated` file is
code {
…
en.wikipedia 1755_BC 6620791 desktop 3 I1N1V1
en.wikipedia 1755_Cape_Ann_earthquake 23516172 desktop 4 E1N1S1U1
en.wikipedia 1755_Cape_Ann_earthquake 23516172 mobile-web 2 F2
en.wikipedia 1755_Lisbon_Earthquake 352339 desktop 1 C1
en.wikipedia 1755_Lisbon_Earthquake 352339 mobile-web 2 B1R1
en.wikipedia 1755_Lisbon_earthquake 352339 desktop 22 D1E1G1J1M1N1O2Q5R1S2T2U2W2
en.wikipedia 1755_Lisbon_earthquake 352339 mobile-web 13 C1I1K1N1O1P2Q1R2T1W1X1
en.wikipedia 1755_Lisbon_earthquake null mobile-app 1 M1
en.wikipedia 1755_Lorbach 16466903 desktop 4 F3X1
en.wikipedia 1755_Meknes_earthquake 72397816 desktop 2 J1N1
en.wikipedia 1755_in_Canada 487717 desktop 1 T1
…
code }

    }


  }
}
links:
