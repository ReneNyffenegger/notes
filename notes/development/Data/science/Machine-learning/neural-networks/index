
Neural networks are *non-linear* → science/mathematics/statistics/model[models].

Applications:
  • Image recognition


{ Neurons

  A neuron can be thought of as a cell that contains a number between 0 and 1 and is able to compute a simple calculation.
  -
  The number is also called *activation*.

}
{ Layers


  There are three types of layers:
    • One input layer
    • n hidden layers
    • One output layer

  Hidden layers and the output layer contain a number of neurons.
  

}
{ Sigmoid function

  σ(x) = 1/(1 + exp(-x))

  For all x, the sigmoid function produced a value between 0 and 1.

  A particularly useful property of the sigmoid function is that that its derivative is
  -
  σ'(x) = σ(x) * (1-σ(x))

}
{ Convolutional neural network (CNN)

  A *convolutional neural network* uses a *convolution operation* rather than a *matrix multiplication* in at least one of its layers.

  CNNs are typically used to analyze visual imagery.

}
{ Recurrent neural network (RNN)

  A *recurrent neural network* is a class of neural networks where connections between nodes can create a cycle.
  -
  Such cycles are essential to remember past events when a sequence is processed (i. e. for implementing a memory).

  RNNs can be used for
    • unsegmented, connected handwriting recognition
    • speech recognition

  RNNs are theoretically Turing complete.

  A neural network without circles in its connections is a *feedforward neural network* (FNN).

}
 Autoencoders #todo-autoencoders

*Autoencoders* is a type of *neural network* used to learn efficient data codings in an unsupervised manner.

 An autoencoder tries to learn the *identity function* (that is: to reconstruct its input). This entails that
 the autoencoder has the equal amount of input and output neurons.
 -
 An additional requirement for an autoencoder is that the number of neurons in the hidden layer must be less than
 the number of input/output neurons. This second requirement forces the autoencoder to learn the most important
 features of the input only.

 An autoencoder consists of
    • The encoding function (*encoder*)
    • The decoding function (*decoder*)
    • A distance function (*loss function*)

 Autoencoders are relevant for → development/Data/quality[data quality].

}
{ TODO

  { Minimal neural network libraries

    → development/Data/science/Machine-learning/neural-networks/libraries/Genann[Genann], written in → development/languages/C-C-plus-plus/C[C].

    → https://github.com/glouw/tinn[tinn] (Tyni neural network) is a dependency free neural network library writtin in C99 using less than 200 lines of code.

    → https://github.com/attractivechaos/kann[kann]

    → https://github.com/dfouhey/caffe64[Caffe64], written in → development/languages/assembler[assembler language] for 64 bit Linux.

  }

}

sa:
 `→ development/languages/Python/libraries/torch/types/nn/Module` is the → development/languages/Python/libraries/torch[PyTorch] base class for all neural network modules.

  The idea of neural networks was first described → development/Artificial-intelligence/history#hist-ai-nn[1943 by McCulloch and Pitts].
