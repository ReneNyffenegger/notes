
Neural networks are *non-linear* → science/mathematics/statistics/model[models].

Applications:
  • Image recognition


{ Neurons

  A neuron can be thought of as a cell that contains a number between 0 and 1 and is able to compute a simple calculation.
  -
  The number is also called *activation*.

}
{ Layers


  There are three types of layers:
    • One input layer
    • n hidden layers
    • One output layer

  Hidden layers and the output layer contain a number of neurons.
  

}
{ Sigmoid function

  σ(x) = 1/(1 + exp(-x))

  For all x, the sigmoid function produced a value between 0 and 1.

  A particularly useful property of the sigmoid function is that that its derivative is
  -
  σ'(x) = σ(x) * (1-σ(x))

}
{ Convolutional neural network (CNN)

  A *convolutional neural network* uses a *convolution operation* rather than a *matrix multiplication* in at least one of its layers.

  CNNs are typically used to analyze visual imagery.

}
{ Feedforward neural network (FNN)

  A neural network without circles in its connections is a *feedforward neural network* (FNN).

}
{ Recurrent neural network (RNN)

  A *recurrent neural network* is a class of neural networks where connections between nodes can create a cycle.
  -
  Such cycles are essential to remember past events when a sequence is processed (i. e. for implementing a memory).

  RNNs can be used for
    • unsegmented, connected handwriting recognition
    • speech recognition

  RNNs are theoretically Turing complete.

  RNNs were already in 2010 widely used in text prediction (usually referred to as *language modeling*).

  A potential drawback of RNNs is that they have an inherently serial structure that prevents them from being run in parallel along the
  sequence length during training and evaluation.
  -
  Also, forward and backward signals need to traverse the full distance of the serial path to reach from one token in the sequence to another (See also Hochreiter et al, 2001)

  { Bidirectional RNN (BiRNN)

    A BiRNN consists of forward and backward RNNs.

    The forward RNN reads the input sequence and calculates a sequence of
    forward hidden states, the backward RNN reads the sequence in the reverse
    order  resulting in a sequence of backward hidden states.

  }
  { Long short-term memory (LSTM) RNNs

    In principle, an arbitrary complex sequence should be able to be generated by a sufficiently large RNN.
    -
    It turns out that standard RNNs don't store past inputs for very long which makes them prone to instability and mistakes, from which
    the RNN cannot easily recover.

    The *long short-term memory (LSTM)* architecture is designed to improve storing and accessing information.

    LSTMs were proposed by S. Hochreiter and J. Schmidhuber in 1997.

  }

}
{ Autoencoders #todo-autoencoders

*Autoencoders* is a type of *neural network* used to learn efficient data codings in an unsupervised manner.

 An autoencoder tries to learn the *identity function* (that is: to reconstruct its input). This entails that
 the autoencoder has the equal amount of input and output neurons.
 -
 An additional requirement for an autoencoder is that the number of neurons in the hidden layer must be less than
 the number of input/output neurons. This second requirement forces the autoencoder to learn the most important
 features of the input only.

 An autoencoder consists of
    • The encoding function (*encoder*)
    • The decoding function (*decoder*)
    • A distance function (*loss function*)

 Autoencoders are relevant for → development/Data/quality[data quality].

}
{ TODO

  { ByteNet

    → https://arxiv.org/abs/1610.10099[Neural Machine Translation in Linear Time]:
    "
      The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. 
    "

  }
  { Minimal neural network libraries

    → development/Data/science/Machine-learning/neural-networks/libraries/Genann[Genann], written in → development/languages/C-C-plus-plus/C[C].

    → https://github.com/glouw/tinn[tinn] (Tyni neural network) is a dependency free neural network library writtin in C99 using less than 200 lines of code.

    → https://github.com/attractivechaos/kann[kann]

    → https://github.com/dfouhey/caffe64[Caffe64], written in → development/languages/assembler[assembler language] for 64 bit Linux.

  }

}

sa:
 `→ development/languages/Python/libraries/torch/types/nn/Module` is the → development/languages/Python/libraries/torch[PyTorch] base class for all neural network modules.

  The idea of neural networks was first described → development/Artificial-intelligence/history#hist-ai-nn[1943 by McCulloch and Pitts].
