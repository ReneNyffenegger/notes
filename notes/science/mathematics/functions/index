$ Math: functions

{ Natural logarithm

code {
log(eⁿ ) =  n
log(e⁰ ) =  0 = log(1)   
log(e¹ ) =  1 = log(e)   
log(e² ) =  2
log(e⁻¹) = -1 = log(1/e) 
code }

code {
log(x * y) = log(x) + log(y)
code}


  gh|math-functions|/img/natural-logarithm.png||

  This plot was created with Python's → development/languages/Python/libraries/matplotlib:
  gh|math-functions|/natural-logarithm.py||

}
{ Logistic function

  The logistic function is a → https://en.wikipedia.org/wiki/Sigmoid_function[sigmoid function] and thus
    • produces a value between 0 and 1
    • is defined for all real input values
    • has a non-negative derivatative at each point
    • exactly one inflection point (which IMHO makes the derivative bell shaped?)

  The inverse of the logistic function is the *logit function*.

  The following plot draws the *standard logistic function*:
  gh|math-functions|/img/standard-logistic-function.png||

  It was created with
  gh|math-functions|/standard-logistic-function.py||

  { TODO

    Sigmoid functions are commonly used as activation funcions in neural networks.

  }

}
{ Logit function

  gh|math-functions|/img/logit.png||

  gh|math-functions|/logit.py||

}
{ softmax

 *softmax* takes k numbers and produces k numbers each of which is between 0 and 1 and whose sum is equal to 1.

  The softmax function is often applied as final function to a neural network.
  
  The following Python script computes the softmax of the values stored in `z`:
  gh|math-functions|/softmax.py||

  When executed, the script prints:
code {
σ_0 = 0.145
σ_1 = 0.053
σ_2 = 0.533
σ_3 = 0.239
σ_4 = 0.029
Sum(σ) = 1.000
code }

}
