<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>HDFS</title>
<link rel="stylesheet" type="text/css" href="file:///home/rene/github/github/notes/out.html//notes.css">
<script src='file:///home/rene/github/github/notes/out.html//q.js'></script>
</head>
<body>
Search notes: <input size='50' id='q' onchange='q();'>
<h1>HDFS</h1>
<div class='t'>
HDFS is designed to solve the <a href='file:///home/rene/github/github/notes/out.html//Linux/filesystem/NFS.html#limitations'>limitations of NFS</a>. It can store large amounts of data. It is fault tolerant (if a machine in the <a href='file:///home/rene/github/github/notes/out.html//development/Apache/Hadoop/cluster.html'>cluster</a> fails, it causes no disruption of service). If too many clients try to access the data resulting in a performance bottleneck, adding more nodes to the cluster remedy it. And finally, HDFS integrates well with <a href='file:///home/rene/github/github/notes/out.html//development/Apache/Hadoop/MapReduce/index.html'>Hadoop MapReduce</a>. </div><div class='g'></div><div class='t'>
HDFS is modelled after GFS (Google Files System). </div>
<div class='h'><h2>Strength</h2>
<div class='t'>
  Files are optimzed for high-throughput sequential reads and writes over large files. </div>
</div>
<div class='h'><h2>Weaknesses</h2>
<div class='t'>
  Inefficient for small files (but the goal was to handle big files anyway). </div><div class='g'></div><div class='t'>
  Lack for (transparent) compression. </div><div class='g'></div><div class='t'>
  No random access to files: data can only appended to files. </div>
</div><hr><p><a href='file:///home/rene/github/github/notes/out.html//index.html'>Index</a><div class='bottom'></div></body>
</html>